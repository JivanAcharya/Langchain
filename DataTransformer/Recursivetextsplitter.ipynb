{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='REaLTabFormer: Generating Realistic Relational\\nand Tabular Data using Transformers\\nAivin V . Solatorio1Olivier Dupriez1\\nAbstract\\nTabular data is a common form of organizing data.\\nMultiple models are available to generate syn-\\nthetic tabular datasets where observations are in-\\ndependent, but few have the ability to produce\\nrelational datasets. Modeling relational data is\\nchallenging as it requires modeling both a “par-\\nent” table and its relationships across tables. We\\nintroduce REaLTabFormer (Realistic Relational\\nand Tabular Transformer), a tabular and relational\\nsynthetic data generation model. It ﬁrst creates a\\nparent table using an autoregressive GPT-2 model,\\nthen generates the relational dataset conditioned\\non the parent table using a sequence-to-sequence\\n(Seq2Seq) model. We implement target masking\\nto prevent data copying and propose the Qδstatis-\\ntic and statistical bootstrapping to detect over-\\nﬁtting. Experiments using real-world datasets\\nshow that REaLTabFormer captures the relational\\nstructure better than a baseline model. REaLTab-\\nFormer also achieves state-of-the-art results on\\nprediction tasks, “out-of-the-box”, for large non-\\nrelational datasets without needing ﬁne-tuning.\\n1. Introduction\\nTabular data is one of the most common forms of data. Many\\ndatasets from surveys, censuses, and administrative sources\\nare provided in this form. These datasets may contain sensi-\\ntive information that cannot be shared openly (Abdelhameed\\net al., 2018). Even when statistical disclosure methods are\\napplied, they may remain vulnerable to malicious attacks\\n(Cheng et al., 2017). As a result, their dissemination is\\nrestricted and the data have limited utility (O’Keefe & Ru-\\nbin, 2015). Differential privacy methods (Ji et al., 2014),\\nhomomorphic encryption approaches (Aslett et al., 2015;\\nWood et al., 2020), or federated machine learning (Yang\\net al., 2019; Lin et al., 2021) may be implemented, allowing\\n1Development Economics Data Group, The World Bank, USA.\\nCorrespondence to:\\nAivin V . Solatorio <asolatorio@worldbank.org >\\nGitHub: @avsolatorio.\\nFigure 1. Illustration of the REaLTabFormer model. The left block\\nshows the non-relational tabular data model using GPT-2 with a\\ncausal LM head. In contrast, the right block shows how a relational\\ndataset’s child table is modeled using a sequence-to-sequence\\n(Seq2Seq) model. The Seq2Seq model uses the observations in\\nthe parent table to condition the generation of the observations in\\nthe child table. The trained GPT-2 model on the parent table, with\\nweights frozen, is also used as the encoder in the Seq2Seq model.\\ninsights from sensitive data to be accessible to researchers.\\nSynthetic tabular data with similar statistical properties as\\nthe real data offer an alternative, offering more value espe-\\ncially in granular and segmentation analyses. To comply\\nwith data privacy requirements, the generative models that\\nproduce these synthetic data must provide guarantees that\\n“data copying” does not happen (Meehan et al., 2020; Car-\\nlini et al., 2023).\\nFormally, tabular data is a collection of observations (rows)\\noithat may or may not be independent. A single obser-\\nvation in a tabular data Twithncolumns is deﬁned by\\noi= [xi1,xi2,...,xij,...,xin], andjindicating the jthcol-\\numn. We refer to tabular data having observations indepen-\\ndent of each other as non-relational tabular data . Tabular\\ndata having observations related to each other are referred to\\nasrelational tabular data . Relational datasets have at least\\none pair of tabular data ﬁles with a one-to-many mapping\\nof observations between the parent table and the child table,arXiv:2302.02041v1  [cs.LG]  4 Feb 2023'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nrespectively, linked by a unique identiﬁer. In the context of a\\nrelational dataset, a parent table is a non-relational tabular\\ndata, whereas the child table is a relational tabular data.\\nRelational tabular databases model the logical partitioning\\nof data and prevent unnecessary duplication of observations\\nfrom the parent to child tables (Jatana et al., 2012). Despite\\nits ubiquity, limited work has been done in generating syn-\\nthetic relational datasets. This may be due to the challenging\\nnature of modeling the complex relationships within and\\nacross tables.\\nThe ﬁeld of synthetic data generation has seen signiﬁcant\\ndevelopment in recent years (Gupta et al., 2016; Abufadda\\n& Mansour, 2021; Hernandez et al., 2022; Figueira & Vaz,\\n2022). Generative models have become mainstream with the\\nadvent of synthetic image generation models such as DALL-\\nE (Ramesh et al., 2021), and most recently, ChatGPT. While\\ngenerative models for images and text are common, mod-\\nels for producing synthetic tabular data are comparatively\\nlimited despite their multiple possible applications. Syn-\\nthetic tabular data can contribute to addressing data privacy\\nissues and data sparseness (Appenzeller et al., 2022). They\\ncan help to make sensitive data accessible to researchers\\n(Goncalves et al., 2020), and to ﬁll gaps in data availabil-\\nity for counterfactual research and agent-based simulations\\n(Fagiolo et al., 2019), and for synthetic control methods\\n(Abadie et al., 2015). Further value can be derived from\\ntabular data by building predictive models using machine\\nlearning (Shwartz-Ziv & Armon, 2022). These predictive\\nmodels can infer variables of interest in the data that may\\notherwise be expensive to collect or correspond to some\\nsuccess metrics that can guide business decisions. Synthetic\\ndata produced by deep learning models have been shown to\\nperform well in predictive modeling tasks. This extends the\\nutility of real-world data that may otherwise be unused due\\nto privacy concerns.\\nThis paper introduces the REalTabFormer , a transformer-\\nbased framework for generating non-relational tabular data\\nand relational datasets. It makes the following contributions:\\nUniﬁed framework The REalTabFormer uses an autore-\\ngressive (GPT-2) transformer to model non-relational tab-\\nular data for modeling and generating parent tables. It\\nthen models and generates observations in the child ta-\\nble using the sequence-to-sequence (Seq2Seq) (Yun et al.,\\n2019) framework. The encoder network uses the pre-trained\\nweights of the network for the parent table, contextualizing\\nthe input for generating arbitrary-length data corresponding\\nto observations in a child table, via the decoder network.\\nStrategies for privacy-preserving training Synthetic\\ndata generation models must not only be able to generate\\nrealistic data but also implement safeguards to prevent the\\nmodel from “memorizing” and copying observations in the\\ntraining data during sampling. We use the distance to clos-est record (DCR), a data-copying measure, and statistical\\nbootstrapping to detect overﬁtting during training robustly.\\nWe introduce target masking for regularization to reduce the\\nlikelihood of training data being replicated by the model.\\nOpen-sourced models We publish the REaLTabFormer\\nmodels as an open-sourced Python package. Install the\\npackage using: pip install realtabformer .1\\nComprehensive evaluation We evaluate the performance\\nof our models on a variety of real-world datasets. We\\nuse open-sourced state-of-the-art models as baselines to\\nassess the performance of REaLTabFormer in generating\\nnon-relational and relational tabular datasets.\\nOur experiments demonstrate the effectiveness of the RE-\\naLTabFormer model for non-relational tabular data, beating\\ncurrent state-of-the-art in machine learning tasks for large\\ndatasets. We further demonstrate that the synthesized obser-\\nvations for the child table generated by the REaLTabFormer\\ncapture relational statistics more accurately than the baseline\\nmodels.\\n2. Related Work\\nRecent advances in deep learning, such as generative adver-\\nsarial networks (Park et al., 2018; Xu et al., 2019; Zhao et al.,\\n2021), autoencoders (Li et al., 2019; Xu et al., 2019; Darabi\\n& Elor, 2021), language models (Borisov et al., 2022), and\\ndiffusion models (Kotelnikov et al., 2022) have been applied\\nto synthetic non-relational tabular data generation. These\\npapers demonstrate deep learning models’ capacity to pro-\\nduce more realistic data than traditional approaches such as\\nBayesian networks (Xu et al., 2019).\\nOn the other hand, generative models for relational datasets\\nare limited (Patki et al., 2016; Gueye et al., 2022). Exist-\\ning models are based on Hierarchical Modeling Algorithms\\n(Patki et al., 2016) where traditional statistical models, Gaus-\\nsian Copulas, are used to learn the joint distributions for\\neach and across tables. While these models can synthesize\\ndata, the quality of the generated data does not accurately\\ncapture the nuanced conditions within and across tables\\n(Fig. 2 and Fig. 3).\\nPadhi et al. (2021) presented TabGPT for generating syn-\\nthetic transactional data. They showed that autoregressive\\ntransformers, particularly GPT, can synthesize arbitrary-\\nlength data. One limitation of TabGPT is that one has\\nto train independent models to produce transactions for\\neach user. This becomes impractical for real-world applica-\\ntions. Our work generalizes the use of GPT by proposing a\\nsequence-to-sequence framework for generating arbitrary-\\nlength synthetic data conditioned on an input.\\n1https://github.com/avsolatorio/\\nREaLTabFormer'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n3. REaLTabFormer\\nREaLTabFormer is a transformer-based framework for gen-\\nerating non-relational tabular data using an autoregres-\\nsive model and relational tabular data using a sequence-\\nto-sequence (Seq2Seq) architecture. The framework also\\nconsists of strategies for encoding tabular data (Section 3.2),\\na statistical method to detect overﬁtting (Section 3.3.2), and\\na constrained sampling strategy during generation.\\nDetails of the framework are described in this section. First,\\nwe present our proposed models to synthesize realistic re-\\nlational datasets. Next, we outline and describe the data\\nprocessing applied to the tabular data as input for train-\\ning the models. We then discuss solutions to improve our\\nmodel’s training and sampling process.\\n3.1. The REaLTabFormer Models\\nParent table model To generate synthetic observations\\nfor a non-relational tabular data T, we model the conditional\\ndistribution of columnar values in each row of the data. Con-\\nsider a single observation oi= [xi1,xi2,...,xij,...,xin]in\\nTas deﬁned earlier. We treat oias a sequence with potential\\ndependencies across values xij, similar to a sentence in a\\ntext. This re-framing provides us with a framework to learn\\nthe conditional distribution xij∼P(X|xi1,xi2,...,xij−1)\\nand sequentially generate the next values in the sequence,\\neventually generating the full observation (Jelinek, 1985;\\nBengio et al., 2000). We use an autoregressive model to\\nlearn this distribution, Fig. 1. In the context of relational\\ndatasets, we use this approach to generate synthetic obser-\\nvations for the parent table T0. We extend this formulation\\nto generate the child table T′—a relational tabular data—\\nassociated with the parent table T0.\\nChild table model The extension is established by in-\\ntroducing a context learned by an encoder network from\\nobservations in T0. Instead of generating oiinT′indepen-\\ndently, we concatenate the child table observations related to\\nthe same observation in T0. This forms an arbitrary-length\\nsequencesi= [o1\\ni,o2\\ni,...,on\\ni], wherenis the number of\\nrelated observations in T′.\\nWe propose to model the generation of sigiven an obser-\\nvation in T0asxn\\nij∼P(X|o1\\ni,...,xn\\ni1,xn\\ni2,...,xn\\nij−1,Ck),\\nwhereCkis a context captured from a related observation\\nin the parent tabular data T0. We also use the same network\\ntrained on the parent table, with weights frozen , as the\\nSeq2Seq model’s encoder. This choice is expected to speed\\nup the training process since only the cross-attention layer\\nand the decoder network are needed to be trained for the\\nchild table model. The encoder network is assumed to have\\nlearned the properties of the parent table and will transfer\\nthis information to the decoder without further ﬁne-tuning\\nits weights, Fig 1.\\nJun\\n2015Jul08 15 22 29 06 13 20 27\\nDate0200040006000800010000Average salesOriginal\\nREaLT abFormer\\nSDVFigure 2. Graph of the daily mean of the Sales variable computed\\nfrom the original Rossmann dataset (blue), synthetic data produced\\nby REaLTabFormer (orange), and data generated by SDV (green).\\nThe REaLTabFormer closely captures the seasonality in the data\\ncompared with the HMA model from the SDV .\\nGPT-2: an autoregressive transformer Previous works\\nhave shown that transformer-based autoregressive models\\ncan capture the conditional distribution of sequential data\\nvery well (Radford et al., 2019; Padhi et al., 2021). RE-\\naLTabFormer uses the GPT-2 architecture—a transformer-\\ndecoder architecture designed for autoregressive tasks—as\\nits base model. We adopt the same architecture for all GPT-2\\ninstances in the framework for simplicity. The GPT-2 archi-\\ntecture used in the REaLTabFormer has 768-dimensional\\nembeddings, 6 decoder layers, and 12 attention heads—a\\nset of parameters similar to DistilGPT2. We use the imple-\\nmentation from the HuggingFace transformers library (Wolf\\net al., 2020).\\n3.2. Tabular Data Encoding\\nThe GReaT model that uses pretrained large language mod-\\nels (LLMs) proposed by Borisov et al. (2022) offers insight\\ninto the minimal data processing requirements for language\\nmodels in generating tabular data. There is, however, the\\npotential for optimization in using autoregressive language\\nmodels for this task, as the ﬁne-tuning process of a large\\npretrained model incurs computational costs. Particularly,\\nLLMs are trained on a large vocabulary where most of the\\ntokens are not needed for generating the tabular data at\\nhand. These unnecessary tokens increase the model’s com-\\nputational requirements and prolong training and sampling\\ntimes. To improve the efﬁciency of our model, we adopt\\na ﬁxed-set vocabulary as initially proposed by Padhi et al.\\n(2021). Generating a ﬁxed vocabulary for each column in\\nthe tabular data offers various advantages in training perfor-\\nmance and sampling. One of the main advantages is being'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n20-30 30-40 40-50 50-60 60-70 70-80 80-90 90+\\nage_group-unknown-\\nAndroid App ...\\nAndroid Phone\\nBlackberry\\nChromebook\\nLinux Desktop\\nMac Desktop\\nT ablet\\nWindows Desktop\\n_rtf_other_\\niPad T ablet\\niPhone\\niPodtouchdevice_typeOriginal\\n20-30 30-40 40-50 50-60 60-70 70-80 80-90 90+\\nage_groupSDV\\n20-30 30-40 40-50 50-60 60-70 70-80 80-90 90+\\nage_groupREaLT abFormer\\n0.00.20.40.60.81.0\\nFigure 3. Joint distributions of the agegroup variable in the parent table and the device type in the child table of the Airbnb test\\ndataset (left), the SDV (middle), and the REalTabFormer (right). The plots show that the REaLTabFormer can synthesize values across the\\ndomain of the variables, while SDV learned only two device types out of thirteen. The REaLTabFormer also generalized and imputed age\\nvalues for users with “iPodtouch” device (red box). This device type group has missing values for age in the original data.\\nable to ﬁlter irrelevant tokens when generating values for\\na speciﬁc column. This directly contributes to efﬁciency\\nin sampling by reducing the chances of generating invalid\\nsamples. Our model performs minimal transformation of\\nthe raw data. First, we identify the various data types for\\neach column in the data. We then perform a series of data\\nprocessing speciﬁc to the column and data type. Notably,\\nwe adopt a fully text-based strategy in handling numeri-\\ncal values. These transformations produce a transformed\\ntabular data used to train the model. Borisov et al. (2022)\\nshowed that variable order has an insigniﬁcant impact on\\nlanguage models, so we did not apply variable permutation.\\nWe discuss the processing for each data type in Appendix A.\\nTraining data for the parent table The GPT-2 model we\\nuse requires a set of token ids as input. To generate these\\nsequences of token ids, we ﬁrst create a vocabulary. This\\nvocabulary maps the unique tokens in each column to a\\nunique token id. Then, for each row in the modiﬁed data,\\nwe apply the mapping in the vocabulary to the tokens. This\\nproduces a list of token ids for each row of the data. The\\nmodel is then trained on an autoregressive task wherein the\\ntarget data corresponds to the right-shifted tokens of the\\ninput data.\\nTraining data for the child table We concatenate the\\ntransformed observations corresponding to related rows in\\nthe child table. A special token is added before and after\\nthe set of tokens representing an individual observation.\\nIn this form, the data we use to train the Seq2Seq model\\ncontains input-output pairs. An input value contains a ﬁxed-\\nlength array of token ids representing the observation in\\nthe parent table. The input is similar to the input used\\nin the parent table model. An arbitrary-length array with\\nthe concatenated token ids for each related observation in\\nthe child table represents the output value. The number\\nof related observations that can be modeled is limited by\\ncomputational resources.3.3. REaLTabFormer Training and Sampling\\nDeep learning models for generative tasks face challenges of\\noverﬁtting the data resulting in issues such as data-copying\\n(Meehan et al., 2020; Carlini et al., 2023). Furthermore,\\nobservations generated by generative models for tabular\\ndata could face issues of validity and inconsistency. These\\nissues in the generated samples impact the efﬁciency of the\\ngenerative process. Our proposed framework addresses the\\naforementioned issues by, (i) introducing a robust statistical\\nmethod to monitor overﬁtting, and (ii) target masking to fur-\\nther reduce the risk of data copying. To improve the rate of\\nproducing valid observations by the model, we also imple-\\nment a constrained generation strategy during the sampling\\nstage.\\n3.3.1. T ARGET MASKING\\nData copying is a critical issue for deep learning-based gen-\\nerative tabular models as it can expose and compromise\\nsensitive information in the training data. To mitigate data-\\ncopying, we introduce target masking . Target masking is a\\nform of regularization aimed at minimizing the likelihood of\\nrecords in the training data being “memorized” and copied\\nby the generative model. Unlike the token masking intro-\\nduced in BERT (Devlin et al., 2018), where input tokens\\nare masked and the model is expected to predict the correct\\ntoken, target masking implements random replacement of\\nthe target or label tokens with a special mask token. This\\nartiﬁcially introduces missing values in the data.\\nWe intend for the model to learn the masks instead of the ac-\\ntual values. During the sampling stage, we then restrict the\\nmask token generation, forcing the model to ﬁll the value\\nwith a valid token probabilistically. Notably, even when the\\nmodel learns to copy the input-output pair, the learned out-\\nput corresponds to the masked version of the input. There-\\nfore, when we process the output, the probabilistic nature of\\nreplacing the mask token reduces the likelihood of generat-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\ning the training data. The mask rate parameter controls the\\nproportion of the tokens that are masked. We use a mask\\nrate of 10% in our experiments.\\n3.3.2. O VERFITTING ASSESSMENT\\nApplying deep learning models to small datasets may easily\\nresult in overﬁtting. This may cause privacy-related issues\\nwhen the model generates observations that are copied from\\nthe training data. Knowing when the model overﬁts the data\\nis also crucial when the purpose is to generate diverse out-of-\\nsample data. An overﬁtted model tends to generate samples\\ngenerally closer to the training data, thereby limiting the\\ngeneralization capacity of the model. While the former issue\\ncan be resolved by post-generation ﬁltering, the latter must\\nbe detected during the model training.\\nTaking hold-out data to detect overﬁtting is a common strat-\\negy in machine learning. Unfortunately, this strategy could\\nresult in the premature termination of model training. It\\nmay also penalize a model based only on a small subset\\nof the data (Blum et al., 1999). The training procedures of\\nexisting state-of-the-art models do not explicitly check for\\noverﬁtting (Xu et al., 2019; Borisov et al., 2022; Kotelnikov\\net al., 2022). We propose and describe below an empiri-\\ncal statistical method to inform the generative model when\\noverﬁtting happens. The method allows for the full data to\\nbe used in the training without the need for a hold-out set.\\nThe design of the method is expected to also help prevent\\ndata copying and the production of data that is riskily close\\nto the training data.\\nDistance to closest record We use the distance to the\\nclosest record (DCR) (Park et al., 2018) to measure the\\nsimilarity of synthetic samples to the original data. The\\nDCR is evaluated by taking a speciﬁed distance metric L\\nbetween the training data Trand the generated data G. We\\nthen ﬁnd the smallest distance for each record. Consider the\\ndistance matrix between TrandG,\\nD=L(Tr,G) (1)\\nThe minimum value in each row iofDis the minimum\\ndistance of the ithrecord in the training data with respect\\nto all records in the generated data. We denote this set of\\nminimum values as ⃗di. The minimum value in each column\\njofDis the minimum distance of the jthrecord in the\\ngenerated data with respect to all records in the training\\ndata. We denote this set of minimum values as ⃗dj. We then\\ntake⃗dg= [⃗di,⃗dj]as the distribution of distances to closest\\nrecords between the training data and the generated data.\\nWe deﬁne the quantity\\n⃗d= [⃗di,⃗dj] (2)\\nas the distance to closest record distribution for some Trandsome arbitrary sample. We also derive the DCR between\\nthe train dataset and some hold-out data Th. Let us denote\\nthis distribution of distances as ⃗dh. We use the distributions\\n⃗dgand⃗dhin our proposed non-parametric method.\\nQuantile difference ( Qδ) statistic Two samples from a\\nsimilar distribution should, on average, have approximately\\nthe same values at each quantile of the distribution. To detect\\nwhether two samples come from different distributions, we\\ndeﬁne a set of quantiles over which we compare the two\\nsamples. For each quantile in the set, we ﬁnd the value at\\nthe given quantile in one sample and measure the proportion\\nof the values in the other sample that are below it. If the\\ndistributions are similar, the proportion should be close to\\nthe given quantile, for all quantiles being tested.\\nFormally, let ShandSghavingmandnobservations, re-\\nspectively, be two samples being compared. Let Qbe a\\nset ofNquantiles, and q∈Qis a speciﬁc quantile in the\\nset. Consider vqas the value in Shat quantileq. Then, we\\ncompute the value\\npq=∑\\nx∈Sg[x≤vq]\\nn(3)\\nwherepqis the proportion of values in Sgthat are less than\\nor equal tovq. We deﬁne a statistic\\nQδ=1\\nN∑\\nq(pq−q) (4)\\nThis formulation has similarities with the Cramer-von Mises\\nω2criterion, but the Qδstatistic has one key difference: the\\nasymmetry of the statistic. This stems from the fact that\\nthe choice of which sample is considered as Sh—the dis-\\ntribution from which vqis identiﬁed—matters. Since we\\nare averaging over the quantiles, this statistic may not yield\\nconclusive guidance for distributions with cumulative dis-\\ntribution functions (CDFs) intersecting at some quantile.\\nNonetheless, this statistic works best in detecting the dissim-\\nilarity of the two samples at the left tail of the distribution\\nwhich matters most for our purpose. This is because the\\ndistributions we are comparing are the DCRs. We want\\nto detect when the distance between the sample and the\\ntraining data is signiﬁcantly closer to zero than expected.\\nWe use theQδstatistic as the basis for detecting overﬁtting.\\nThe threshold against which this statistic will be compared\\nduring training is produced through an empirical bootstrap-\\nping over random samples from the training data. The\\ndetails of the bootstrapping method are explained next.\\nQδstatistic threshold via bootstrapping We use three\\nhyperparameters in estimating the threshold that will signal\\nwhen overﬁtting occurs during training. First, a sample pro-\\nportionρcorresponds to a fraction of the training data. This'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 1. Machine learning efﬁcacy (MLE) and discriminator measure (DM) evaluated on the synthetic data generated by the models\\n(columns) trained on six real-world datasets (rows): Abalone (AB), Adult income (AD), Buddy (BU), California housing (CA), Diabetes\\n(DI), and Facebook Comments (FB). The MLE is measured by the R2for regression, while macro average F1is used for classiﬁcation\\ntasks; higher scores are better. A discriminator measure closer to 50% is better. Best scores are highlighted for the MLE measure,\\nconsidering standard deviation. No reported results for GReaT on the FB dataset due to impractical training time.\\nOriginal TV AE CTABGAN+ Tab-DDPM GReaT REaLTabFormer\\nAB (R2)MLE ( ↑) 0.5562 ±0.004 0.3943 ±0.012 0.4697 ±0.014 0.5248 ±0.011 0.3530 ±0.031 0.5035 ±0.011\\nDM ( ↓) - 82.96 ±2.42 75.64±1.20 59.88±2.22 70.46±0.92 63.08±1.18\\nAD (F1)MLE 0.8155 ±0.001 0.7695 ±0.004 0.7778 ±0.003 0.7922 ±0.002 0.7997 ±0.002 0.8113 ±0.002\\nDM - 95.48 ±1.34 61.17±0.40 53.73±0.22 68.04±0.26 55.78±0.20\\nBU (F1)MLE 0.9303 ±0.002 0.9233 ±0.002 0.9267 ±0.002 0.9057 ±0.003 0.9279 ±0.003 0.9278 ±0.003\\nDM - 66.56 ±0.56 58.33±0.49 54.43±0.47 62.18±0.45 55.86±0.47\\nCA (R2)MLE 0.8568 ±0.001 0.7373 ±0.004 0.5231 ±0.006 0.8252 ±0.002 0.7189 ±0.004 0.8076 ±0.003\\nDM - 62.06 ±0.60 90.14±1.03 54.30±0.89 66.78±0.47 57.29±0.56\\nDI (F1)MLE 0.7759 ±0.014 0.7395 ±0.035 0.7339 ±0.024 0.7448 ±0.031 0.7419 ±0.03 0.7315 ±0.027\\nDM - 90.16 ±1.31 70.94±1.99 69.00±1.56 74.88±1.79 75.56±2.84\\nFB (R2)MLE 0.8371 ±0.001 0.6374 ±0.007 0.4722 ±0.053 0.6850 ±0.006 - 0.7702 ±0.004\\nDM - 97.72 ±0.80 93.60±0.28 66.07±0.23 - 65.46 ±0.83\\nfraction will be randomly sampled during the bootstrapping\\nand evaluation phases of the generative model training. Sec-\\nond, theαvalue for choosing the critical threshold for the\\nbootstrap statistic. Third, we specify a bootstrap round B\\ncorresponding to the number of times we compute the Qδ\\nstatistic between three random samples—two, each of size\\nρ, and the rest having size 1−2ρof the training data.\\nFormally, for a given training data TrwithNobservations,\\nwe deﬁne a bootstrap method to generate a conﬁdence in-\\nterval for the Qδstatistic speciﬁc to the tabular data at hand.\\nFor each bootstrap round b∈B, we take three random\\nsamplesStr,Sh, andSg, without replacement. ShandSg\\nare each of size ρN, whileStrcontains (1−2ρ)Nsamples.\\nWe compute the DCR distributions ⃗dgand⃗dhfor the two\\nsamplesShandSg, respectively, relative to sample Str. We\\nthen compute the Qδstatistic between ⃗dgand⃗dh, where we\\ntake⃗dhas the distribution from which we compute the value\\nvqin Equation 3. We store the statistic computed across the\\nbootstrap rounds. We use the speciﬁed αto get the cutoff\\nvalue that will be used as the statistic threshold. We use\\nthis threshold Q′\\nδduring training to compare the Qδstatistic\\nderived from the generated samples by the model. We set\\nρ= 0.165,α= 0.95, andB= 500 in our experiments.\\nEarly stopping with Q′\\nδOur training procedure is paused\\nat each epoch that is a multiple of E. We generate data from\\nthe model during these epochs. The generated data has size\\nSg. We then take two mutually exclusive random samples\\nfrom the training data, without replacement, to represent\\nStrandSh. We compute the Qe\\nδfor this epoch based on the\\nsamples generated and drawn. Then, we compare this statis-\\ntic to the previously computed threshold Q′\\nδ. We continuetraining the model if Qe\\nδ< Q′\\nδ. We save a checkpoint of\\nthis model. We terminate the model training when Qe\\nδ>Q′\\nδ\\nforXconsecutive epochs. We then load the checkpoint for\\nthe most recent model that satisﬁed the condition Qe\\nδ<Q′\\nδ.\\nIn our experiments, we set E= 5as the period of our over-\\nﬁtting evaluation and X= 2 as our grace period before\\ntraining termination.\\n3.3.3. S AMPLING\\nThe models we use build each observation sequentially,\\none token at a time. We leverage the structure of our data\\nprocessing to optimize the generation of samples from the\\ntrained models. Using a vocabulary speciﬁc to a column in\\nthe input data allows us to implement a constrained genera-\\ntion of tokens for each column.\\nWe track the token ids that form the domain of each column\\nduring the generation of the vocabulary using a hash map.\\nBased on this, the tokens that are invalid for the columns will\\nnot be considered for generation in the timestep representing\\nthe column. This strategy allows for efﬁcient sampling\\nwherein the likelihood of generating an invalid sample is\\nclose to zero. In our experiments, ≪1%invalid samples\\nare generated during the sampling phase.\\n4. Experiments and Results\\nThis section outlines the evaluation process we conducted to\\nquantify the performance of the proposed REaLTabFormer\\nframework compared with baseline models. We ﬁrst demon-\\nstrate that the performance of the model we use to generate\\nthe parent tables, and non-relational tabular data in gen-\\neral, compares with or exceeds the performance of state-of-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 2. Logistic detection (LD) measure using random forest\\nmodel for the generated parent, child, and merged tables by the\\nHierarchical Modeling Algorithm (HMA) from SDV and the RE-\\naLTabFormer (RTF) models. Our model consistently beats the\\nSDV model on this metric.\\nDATASET TABLE SDV RTF\\nROSSMANNPARENT 31.77 ±3.41 81.04 ±4.54\\nCHILD 6.53±0.39 52.08 ±0.89\\nMERGED 2.80±0.25 28.33 ±2.31\\nAIRBNBPARENT 7.37±0.72 89.65 ±1.92\\nCHILD 0.00±0.00 30.48 ±0.79\\nMERGED 0.00±0.00 21.43 ±1.10\\nthe-art models in real-world tabular data generation tasks\\nmeasured by the machine learning efﬁcacy metric. We also\\nuse the discriminator measure to quantify how realistic the\\nsamples generated by each model are. We proceed to model\\nreal-world relational datasets and show, quantitatively using\\nlogistic detection, that the synthetic data produced by the\\nREaLTabFormer are more realistic and accurate.\\n4.1. Data\\nWe use a collection of real-world datasets, listed in Table 3,\\ncommonly used in previous works for non-relational tabular\\ndata generation (Xu et al., 2019; Zhao et al., 2021; Gor-\\nishniy et al., 2021; Borisov et al., 2022; Kotelnikov et al.,\\n2022). These datasets differ with respect to the number of\\nobservations, ranging from 768 up to 197,080 observations.\\nThere is also variation in the number of variables they con-\\ntain, ranging from 8 to 50 numerical variables and 0 up to 8\\ncategorical variables. The datasets cover regression, binary,\\nand multi-class classiﬁcation prediction tasks.\\nWe use two real-world datasets to compare the performance\\nof the REaLTabFormer on modeling relational tabular data\\ncompared with the baseline. These datasets are the Ross-\\nmann dataset and the Airbnb dataset used in prior work on\\nsynthetic relation data generation (Patki et al., 2016).\\n4.2. Baseline models\\nNon-relational tabular data We use models that apply\\ndifferent deep learning architectures for generating non-\\nrelational tabular data as baselines to compare the REaLTab-\\nFormer model with. The TV AE is based on variational\\nautoencoder (Xu et al., 2019), the CTABGAN+ on GAN\\narchitecture (Zhao et al., 2022), the Tab-DDPM on diffusion\\n(Kotelnikov et al., 2022), and GReaT uses pretrained LLM\\n(Borisov et al., 2022).\\nRelational datasets Models for generating relational\\ndatasets are limited. Gueye et al. (2022) published work on\\nusing GAN for relational datasets but no open-sourced im-\\na b c d\\nStoreType0200040006000800010000SalesData\\nOriginal\\nREaLT abFormer\\nSDVFigure 4. Summary of the average “Sales” variable in the child\\ntable of the Rossmann dataset grouped by “StoreType” variable in\\nthe parent table. The values shown are from the original data (blue),\\nsynthetic data produced by REaLTabFormer (orange), and data\\ngenerated by SDV (green). This graph shows that REaLTabFormer\\ncaptures the inter-table variations and relationships well.\\nplementation is available. We choose to limit our baselines\\nto open-sourced models; hence, we only use the Hierarchi-\\ncal Modeling Algorithm (HMA) available in the Synthetic\\nData Vault (SDV) as our baseline (Patki et al., 2016).\\n4.3. Generative models training\\nThe GReaT model was trained for 100 epochs for each\\ndata. The parameters for the TV AE, CTABGAN+, and\\nTab-DDPM models had been tuned for the predictive task\\nitself using the real validation data from Kotelnikov et al.\\n(2022). For the relational datasets, we trained the HMA\\nmodel as prescribed in the SDV documentation. In contrast,\\nthe REaLTabFormer model was not tuned against any of\\nthe machine learning tasks. The model solely relied on the\\noverﬁtting metric discussed in Section 3.3.2. We used the\\nsame parameters for the different datasets to test how the\\nREaLTabFormer performs “out-of-the-box”.\\n4.4. Measures and Results\\nWe select the following measures to quantify the quality and\\nutility of the generated samples by the generative models.\\nMachine Learning (ML) efﬁcacy The machine learning\\n(ML) efﬁcacy (Xu et al., 2019; Kotelnikov et al., 2022;\\nBorisov et al., 2022) measures the potential utility of the\\nsynthetic data to supplant the real data for machine learn-\\ning tasks, in particular, training a prediction model. The\\nML efﬁcacy reported by Borisov et al. (2022) in their work\\nused ML models that were not ﬁne-tuned. Kotelnikov et al.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n(2022) showed that the ML efﬁcacy computed from models\\nthat are not ﬁne-tuned may show spurious results. They in-\\nstead optimized the ML models—CatBoost (Prokhorenkova\\net al., 2018)—they used in reporting the ML efﬁcacy. This\\napproach is closer to what researchers are expected to do\\nin the real world, therefore, we adopt these tuned models\\nin our experiments. We generate a validation set from the\\ngenerative models to signal the early-stopping condition\\nduring the ML model training. This is in contrast with the\\nmethod used by Kotelnikov et al. (2022) where they still\\nrelied on the real validation data for the early-stopping of\\nthe ML model.\\nWe report the macro average F1 score (Opitz & Burst, 2019)\\nfor classiﬁcation tasks and the R2metric for regression\\ntasks. Our results presented in Table 1 (MLE) show that\\nREaLTabFormer, despite not being ﬁne-tuned, produces ML\\nefﬁcacy scores that are the best or second-best compared\\nwith the baselines. This demonstrates that REaLTabFormer\\ncan be used, “out-of-the-box”, to generate synthetic data\\nwith state-of-the-art performance in machine learning tasks.\\nThe FB comments dataset, where REaLTabFormer obtained\\nthe best performance, is the largest dataset tested and has\\nthe largest number of columns. Training the GReaT model\\non this dataset yielded impractical runtime so no result is\\nreported. This supports our view that using LLM trained\\non a large vocabulary, containing a majority of irrelevant\\ntokens, limits the efﬁciency of the model.\\nDiscriminator measure We adopt the discriminator mea-\\nsure (Borisov et al., 2022) to quantify whether the data\\ngenerated by a model is easily distinguishable from real\\ndata. A dataset is made by combining an equal number of\\nreal and synthetic data. Real observations in this dataset are\\nlabeled as “1” and synthetic observations are labeled as “0”.\\nSimilar to Borisov et al. (2022), we train a random forest\\nmodel to predict the labels given an observation. A held-out\\ndataset containing a combination of synthetic samples and\\nreal test data is then used to report the ﬁnal measure.\\nAn accuracy that is closer to 50% implies better synthetic\\ndata quality since the discriminative model is not able to\\ndistinguish the real from the synthetic observations. We\\nreport our results in Table 1 (DM). The DM measure shows\\nthat the Tab-DDPM has the most indistinguishable synthetic\\ndata. Nonetheless, REaLTabFormer, without the need for\\ntuning, has DM measures that are close to the Tab-DDPM.\\nThis suggests that the synthetic data produced by a diffusion-\\nbased model and REaLTabFormer are realistic compared\\nwith the other baselines.\\nLogistic Detection For relational datasets, we use logistic\\ndetection (LD) (Fisher et al., 2019; Gueye et al., 2022) to\\nquantify the quality of the parent, child, and merged tables\\ngenerated by REaLTabFormer compared with the HMAmodel. We evaluate ROC-AUC scores averaged over (N=3)\\ncross-validation folds,\\nµRA=1\\nNN∑\\ni=1max(0.5,ROC−AUC)×2−1 (5)\\nThe value reported is LD= 100×(1−µRA), where scores\\nrange from 0 to 100, and scores closer to 100 imply better\\nsynthetic data quality. We use random forest in measuring\\nthe logistic detection instead of the standard logistic regres-\\nsion model. The random forest model captures non-linearity\\nin the data well than logistic regression (Couronn ´e et al.,\\n2018), reducing the likelihood of spurious results. We report\\nthe results in Table 2. Additional LD results using logistic\\nregression are shown in Table 4.\\nThe REaLTabFormer model produces signiﬁcantly higher-\\nquality synthetic data than the HMA model across the\\ndatasets tested. The high values of LD for the child and\\nthe merged tables highlight the ability of REaLTabFormer\\nto accurately synthesize relational datasets in comparison\\nwith the leading baseline. The LD metric shows that data\\ngenerated by SDV for the Airbnb child table is entirely dis-\\ntinguishable from the real data. The quantitative results are\\nsupported by relational statistics computed from synthetic\\ndatasets produced by REaLTabFormer and the HMA model:\\nFigures 2 to 4.\\n5. Conclusion\\nWe presented REaLTabFormer, a framework capable of\\ngenerating high-quality non-relational tabular data and re-\\nlational datasets. This work extends the application of\\nsequence-to-sequence models to modeling and generating\\nrelational datasets. We introduced target masking as a com-\\nponent in the model to mitigate data-copying and safeguard-\\ning from potentially sensitive data leaking from the training\\ndata. We proposed a statistical method and the Qδstatistic\\nfor detecting overﬁtting in model training. This statistical\\nmethod may be adapted to other generative model training.\\nWe showed that our proposed model generates realistic syn-\\nthetic tabular data that can be a proxy for real-world data in\\nmachine learning tasks. REaLTabFormer’s ability to model\\nrelational datasets accurately compared with existing open-\\nsourced alternative contributes to solving existing gaps in\\ngenerative models for realistic relational datasets. Finally,\\nthis work can be extended and applied to data imputation,\\ncross-survey imputation, and upsampling for machine learn-\\ning with imbalanced data. A BERT-like encoder can be used\\ninstead of GPT-2 with the REaLTabFormer for modeling\\nrelational datasets. We also see opportunities to improve\\nprivacy protection strategies and the development of more\\ncomponents like target masking embedded into synthetic\\ndata generation models to prevent sensitive data exposure.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n6. REaLTabFormer Python Package\\nWe publish the REaLTabFormer as a package on PyPi. We\\nshow below how the model can be easily trained on any\\ntabular dataset, loaded as a Pandas DataFrame.\\n6.1. Non-relational tabular model\\nUse the following snippet to ﬁt the REaLTabFormer on a\\nnon-relational tabular dataset. One can control the various\\nhyper-parameters of the model and the ﬁtting method, e.g.,\\nthe number of bootstrap rounds numbootstrap , the frac-\\ntion of training data frac used to generate the Qδstatistic,\\netc. Keyword arguments for the HuggingFace transformers\\nTrainer class can also be passed as **kwargs when\\ninitializing the model.\\n1# pip install realtabformer\\n2import pandas as pd\\n3from realtabformer import REaLTabFormer\\n4\\n5# NOTE: Remove any unique identifiers in the\\n6# data that you don\\'t want to be modeled.\\n7df = pd.read_csv(\"foo.csv\")\\n8\\n9# Non-relational or parent table.\\n10rtf_model = REaLTabFormer(\\n11 model_type=\"tabular\",\\n12 gradient_accumulation_steps=4)\\n13\\n14# Fit the model on the dataset.\\n15# Additional parameters can be\\n16# passed to the /grave.ts1.fit /grave.ts1method.\\n17rtf_model.fit(df)\\n18\\n19# Save the model to the current directory.\\n20# A new directory /grave.ts1rtf_model/ /grave.ts1will be created.\\n21# In it, a directory with the model\\'s\\n22# experiment id /grave.ts1idXXXX /grave.ts1will also be created\\n23# where the artefacts of the model will be stored.\\n24rtf_model.save(\"rtf_model/\")\\n25\\n26# Generate synthetic data with the same\\n27# number of observations as the real dataset.\\n28samples = rtf_model.sample(n_samples=len(df))\\n29\\n30# Load the saved model. The directory to the\\n31# experiment must be provided.\\n32rtf_model2 = REaLTabFormer.load_from_dir(\\n33 path=\"rtf_model/idXXXX\")\\n6.2. Non-relational tabular model\\nREaLTabFormer for relational databases requires a two-\\nphase training. First, the model for the parent table is trained\\nas a non-relational tabular data, then saved. Second, we\\npass the path of the saved parent model when creating the\\nREaLTabFormer instance for the child model to be used as\\nits encoder, then train. Generate synthetic samples from the\\nparent table and use as input to the trained child model to\\ngenerate the synthetic relational observations.1# pip install realtabformer\\n2import os\\n3import pandas as pd\\n4from realtabformer import REaLTabFormer\\n5\\n6pdir = Path(\"rtf_parent/\")\\n7parent_df = pd.read_csv(\"foo.csv\")\\n8child_df = pd.read_csv(\"bar.csv\")\\n9join_on = \"unique_id\"\\n10\\n11# Make sure that the key columns in both the\\n12# parent and the child table have the same name.\\n13assert ((join_on inparent_df.columns) and\\n14 (join_on inchild_df.columns))\\n15\\n16# Non-relational or parent table. Don\\'t include the\\n17# unique_id field.\\n18parent_model = REaLTabFormer(model_type=\"tabular\")\\n19parent_model.fit(parent_df.drop(join_on, axis=1))\\n20parent_model.save(pdir)\\n21\\n22# # Get the most recently saved parent model,\\n23# # or a specify some other saved model.\\n24# parent_model_path = pdir / \"idXXX\"\\n25parent_model_path = sorted([\\n26 pfor pinpdir.glob(\"id *\")ifp.is_dir()],\\n27 key=os.path.getmtime)[-1]\\n28\\n29child_model = REaLTabFormer(\\n30 model_type=\"relational\",\\n31 parent_realtabformer_path=parent_model_path,\\n32 output_max_length= None , train_size=0.8)\\n33\\n34child_model.fit(\\n35 df=child_df, in_df=parent_df, join_on=join_on)\\n36\\n37# Generate parent samples.\\n38parent_samples = parent_model.sample(len(parend_df))\\n39\\n40# Create the unique ids based on the index.\\n41parent_samples.index.name = join_on\\n42parent_samples = parent_samples.reset_index()\\n43\\n44# Generate the relational observations.\\n45child_samples = child_model.sample(\\n46 input_unique_ids=parent_samples[join_on],\\n47 input_df=parent_samples.drop(join_on, axis=1),\\n48 gen_batch=64)\\nAcknowledgments This project was supported by the\\n“Enhancing Responsible Microdata Access to Improve\\nPolicy and Response in Forced Displacement Situations”\\nproject funded by the World Bank-UNHCR Joint Data Cen-\\nter on Forced Displacement (JDC) – KP-P174174-GINP-\\nTF0B5124. We also thank Patrick Brock for providing\\ninsightful comments. The ﬁndings, interpretations, and\\nconclusions expressed in this paper are entirely those of\\nthe authors. They do not necessarily represent the views\\nof the International Bank for Reconstruction and Develop-\\nment/World Bank and its afﬁliated organizations, or those\\nof the Executive Directors of the World Bank or the govern-\\nments they represent.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nReferences\\nAbadie, A., Diamond, A., and Hainmueller, J. Compara-\\ntive politics and the synthetic control method. American\\nJournal of Political Science , 59(2):495–510, 2015.\\nAbdelhameed, S. A., Moussa, S. M., and Khalifa, M. E.\\nPrivacy-preserving tabular data publishing: a comprehen-\\nsive evaluation from web to cloud. Computers & Security ,\\n72:74–95, 2018.\\nAbufadda, M. and Mansour, K. A survey of synthetic data\\ngeneration for machine learning. In 2021 22nd Inter-\\nnational Arab Conference on Information Technology\\n(ACIT) , pp. 1–7. IEEE, 2021.\\nAppenzeller, A., Leitner, M., Philipp, P., Krempel, E., and\\nBeyerer, J. Privacy and utility of private synthetic data for\\nmedical data analyses. Applied Sciences , 12(23):12320,\\n2022.\\nAslett, L. J., Esperan c ¸a, P. M., and Holmes, C. C. A re-\\nview of homomorphic encryption and software tools for\\nencrypted statistical machine learning. arXiv preprint\\narXiv:1508.06574 , 2015.\\nBengio, Y ., Ducharme, R., and Vincent, P. A neural proba-\\nbilistic language model. Advances in neural information\\nprocessing systems , 13, 2000.\\nBlum, A., Kalai, A., and Langford, J. Beating the hold-out:\\nBounds for k-fold and progressive cross-validation. In\\nProceedings of the twelfth annual conference on Compu-\\ntational learning theory , pp. 203–208, 1999.\\nBorisov, V ., Seßler, K., Leemann, T., Pawelczyk, M., and\\nKasneci, G. Language Models are Realistic Tabular Data\\nGenerators, October 2022. arXiv:2210.06280 [cs].\\nCarlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag,\\nV ., Tram `er, F., Balle, B., Ippolito, D., and Wallace, E.\\nExtracting training data from diffusion models. arXiv\\npreprint arXiv:2301.13188 , 2023.\\nCheng, L., Liu, F., and Yao, D. Enterprise data breach:\\ncauses, challenges, prevention, and future directions. Wi-\\nley Interdisciplinary Reviews: Data Mining and Knowl-\\nedge Discovery , 7(5):e1211, 2017.\\nCouronn ´e, R., Probst, P., and Boulesteix, A.-L. Random\\nforest versus logistic regression: a large-scale benchmark\\nexperiment. BMC bioinformatics , 19:1–14, 2018.\\nDarabi, S. and Elor, Y . Synthesising multi-modal\\nminority samples for tabular data. arXiv preprint\\narXiv:2105.08204 , 2021.Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805 ,\\n2018.\\nFagiolo, G., Guerini, M., Lamperti, F., Moneta, A., and\\nRoventini, A. Validation of agent-based models in eco-\\nnomics and ﬁnance. In Computer simulation validation ,\\npp. 763–787. Springer, 2019.\\nFigueira, A. and Vaz, B. Survey on synthetic data generation,\\nevaluation methods and gans. Mathematics , 10(15):2733,\\n2022.\\nFisher, C. K., Smith, A. M., and Walsh, J. R. Machine\\nlearning for comprehensive forecasting of alzheimer’s\\ndisease progression. Scientiﬁc reports , 9(1):1–14, 2019.\\nGoncalves, A., Ray, P., Soper, B., Stevens, J., Coyle, L.,\\nand Sales, A. P. Generation and evaluation of synthetic\\npatient data. BMC medical research methodology , 20(1):\\n1–40, 2020.\\nGorishniy, Y ., Rubachev, I., Khrulkov, V ., and Babenko,\\nA. Revisiting deep learning models for tabular data. Ad-\\nvances in Neural Information Processing Systems , 34:\\n18932–18943, 2021.\\nGueye, M., Attabi, Y ., and Dumas, M. Row conditional-\\ntgan for generating synthetic relational databases. arXiv\\npreprint arXiv:2211.07588 , 2022.\\nGupta, A., Vedaldi, A., and Zisserman, A. Synthetic data\\nfor text localisation in natural images. In Proceedings\\nof the IEEE conference on computer vision and pattern\\nrecognition , pp. 2315–2324, 2016.\\nHernandez, M., Epelde, G., Alberdi, A., Cilla, R., and\\nRankin, D. Synthetic data generation for tabular health\\nrecords: A systematic review. Neurocomputing , 2022.\\nJatana, N., Puri, S., Ahuja, M., Kathuria, I., and Gosain, D.\\nA survey and comparison of relational and non-relational\\ndatabase. International Journal of Engineering Research\\n& Technology , 1(6):1–5, 2012.\\nJelinek, F. Markov source modeling of text generation. In\\nThe impact of processing techniques on communications ,\\npp. 569–591. Springer, 1985.\\nJi, Z., Lipton, Z. C., and Elkan, C. Differential privacy and\\nmachine learning: a survey and review. arXiv preprint\\narXiv:1412.7584 , 2014.\\nKotelnikov, A., Baranchuk, D., Rubachev, I., and Babenko,\\nA. Tabddpm: Modelling tabular data with diffusion mod-\\nels.arXiv preprint arXiv:2209.15421 , 2022.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nLi, S.-C., Tai, B.-C., and Huang, Y . Evaluating variational\\nautoencoder as a private data release mechanism for tab-\\nular data. In 2019 IEEE 24th Paciﬁc Rim International\\nSymposium on Dependable Computing (PRDC) , pp. 198–\\n1988. IEEE, 2019.\\nLin, J., Ma, J., and Zhu, J. Privacy-preserving house-\\nhold characteristic identiﬁcation with federated learning\\nmethod. IEEE Transactions on Smart Grid , 13(2):1088–\\n1099, 2021.\\nMeehan, C., Chaudhuri, K., and Dasgupta, S. A non-\\nparametric test to detect data-copying in generative mod-\\nels. In International Conference on Artiﬁcial Intelligence\\nand Statistics , 2020.\\nO’Keefe, C. M. and Rubin, D. B. Individual privacy versus\\npublic good: protecting conﬁdentiality in health research.\\nStatistics in medicine , 34(23):3081–3103, 2015.\\nOpitz, J. and Burst, S. Macro f1 and macro f1. arXiv\\npreprint arXiv:1911.03347 , 2019.\\nPadhi, I., Schiff, Y ., Melnyk, I., Rigotti, M., Mroueh, Y .,\\nDognin, P., Ross, J., Nair, R., and Altman, E. Tabular\\nTransformers for Modeling Multivariate Time Series. In-\\nstitute of Electrical and Electronics Engineers Inc., June\\n2021. doi: 10.1109/ICASSP39728.2021.9414142. ISSN:\\n15206149.\\nPark, N., Mohammadi, M., Gorde, K., Jajodia, S., Park,\\nH., and Kim, Y . Data synthesis based on generative\\nadversarial networks. arXiv preprint arXiv:1806.03384 ,\\n2018.\\nPatki, N., Wedge, R., and Veeramachaneni, K. The Synthetic\\nData Vault. In 2016 IEEE International Conference on\\nData Science and Advanced Analytics (DSAA) , pp. 399–\\n410, 2016. doi: 10.1109/DSAA.2016.49.\\nProkhorenkova, L., Gusev, G., V orobev, A., Dorogush, A. V .,\\nand Gulin, A. Catboost: unbiased boosting with categori-\\ncal features. Advances in neural information processing\\nsystems , 31, 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language Models are Unsupervised Multi-\\ntask Learners. 2019.\\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Rad-\\nford, A., Chen, M., and Sutskever, I. Zero-shot text-\\nto-image generation. In International Conference on\\nMachine Learning , pp. 8821–8831. PMLR, 2021.\\nShwartz-Ziv, R. and Armon, A. Tabular data: Deep learning\\nis not all you need. Information Fusion , 81:84–90, 2022.Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,\\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\\nY ., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M.,\\nLhoest, Q., and Rush, A. Transformers: State-of-the-Art\\nNatural Language Processing. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Language\\nProcessing: System Demonstrations , pp. 38–45, Online,\\nOctober 2020. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2020.emnlp-demos.6.\\nWood, A., Najarian, K., and Kahrobaei, D. Homomorphic\\nencryption for machine learning in medicine and bioin-\\nformatics. ACM Computing Surveys (CSUR) , 53(4):1–35,\\n2020.\\nXu, L., Skoularidou, M., Cuesta-Infante, A., and Veera-\\nmachaneni, K. Modeling tabular data using conditional\\nGAN. In Proceedings of the 33rd International Confer-\\nence on Neural Information Processing Systems , number\\n659, pp. 7335–7345. Curran Associates Inc., Red Hook,\\nNY , USA, December 2019.\\nYang, Q., Liu, Y ., Chen, T., and Tong, Y . Federated machine\\nlearning: Concept and applications. ACM Transactions\\non Intelligent Systems and Technology (TIST) , 10(2):1–19,\\n2019.\\nYun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and\\nKumar, S. Are transformers universal approximators\\nof sequence-to-sequence functions? arXiv preprint\\narXiv:1912.10077 , 2019.\\nZhao, Z., Kunar, A., Birke, R., and Chen, L. Y . Ctab-gan:\\nEffective table data synthesizing. In Asian Conference on\\nMachine Learning , pp. 97–112. PMLR, 2021.\\nZhao, Z., Kunar, A., Birke, R., and Chen, L. Y . Ctab-\\ngan+: Enhancing tabular data synthesis. arXiv preprint\\narXiv:2204.00401 , 2022.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nA. Raw data processing\\nNumerical data Various methods have been proposed for\\nrepresenting numerical data as input to generative models\\nin the context of tabular data. The CTGAN and TV AE\\nmodels suggest the use of gaussian mixture models to en-\\ncode numerical values (Xu et al., 2019). On the other hand,\\nthe TabFormer model introduced quantization as a way to\\nencode numeric data (Padhi et al., 2021). However, these\\napproaches are lossy. As argued by Borisov et al. (2022),\\nthese lossy transformations may not be optimal.\\nIn our model, we adopt a fully text-based strategy in han-\\ndling numerical values. We apply a sequence of transforma-\\ntions that converts a column of numeric value into, possibly,\\nmulti-columnar data. We use the following transformation\\nof numerical columns. We also show the outcome of each\\ntransformation step on the sample numerical series below.\\nFor illustration, this example numerical-valued series\\n[1032.325345,10.291,-3.0]\\nis converted into\\n[“10”,“32”,“.3”,“3”]\\n[“00”,“10”,“.2”,“9”]\\n[“-0”,“03”,“.0”,“0”]\\n•We set a rounding resolution to normalize the size of\\nthe numerical values. For example, round to at most 2\\ndecimal places.\\n–[1032.33,10.29,-3.0]\\n• We then cast the values to string.\\n–[“1032.33”,“10.29”,“-3.0”]\\n•We identify the magnitude of the most signiﬁcant digit\\nof the largest value in the column by looking for the\\nlocation of the decimal point of the largest value. The\\nmagnitude of the most signiﬁcant digit for the largest\\nvalue in the example is 4.\\n–[“1032.33”,“10.29”,“-3.0”]\\n•We use the magnitude to left-align all the other values\\nin the data by padding them with leading zeros.\\n–[“1032.33”,“0010.29”,“00-3.0”]\\n•We then take the length of the longest string after this\\ntransformation and left-justify the data by padding ze-\\nros to the right of the values that are shorter than the\\nlongest string.\\n–[“1032.33”,“0010.29”,“00-3.00”]•Then, the negative sign for negative values is trans-\\nposed to the leftmost part of the string.\\n–[“1032.33”,“0010.29”,“-003.00”]\\n•Note that for integral values, we only perform the left\\nalignment by padding the values with leading zeros.\\nAfter this series of transformations, we tokenize the values\\ninto ﬁxed-length partitions. For the same example values,\\nsay we choose the partition size to be 2, we get the following\\ntokenized table.\\n[“10”,“32”,“.3”,“3”]\\n[“00”,“10”,“.2”,“9”]\\n[“-0”,“03”,“.0”,“0”]\\nThis transformation is done to mitigate the explosion of the\\nvocabulary if the numeric values are all distinct. We found\\nin our experiments that using single-character partitioning\\nworks best. We suppose that this effect is attributable to the\\ninherent regularization of generating an entire sequence of\\nnumbers one digit at a time.\\nDatetime data For date or time data types, we ﬁrst per-\\nform a transformation of the raw data into Unix timestamp\\nrepresentation. This representation is then treated as regular\\nnumeric data; hence, we apply the data processing discussed\\nfor numeric data types.\\nCategorical data Unique values in categorical columns\\nare treated as unique tokens in the vocabulary. No additional\\nprocessing is done.\\nMissing values No transformation is done for missing\\nvalues present in the data. We let the model learn the dis-\\ntribution of the missing values. This strategy gives us the\\nﬂexibility to let the model impute or generate missing values\\nduring the sampling process.\\nInput data aggregation As illustrated above, the trans-\\nformation of numerical data types expands the dataset by\\npartitioning the string version of the values. As such, we\\ncombine the processed columns into modiﬁed tabular data.\\nWe use this modiﬁed tabular data as input for our models.\\nEach unique value in the new columns in this data will be\\nmapped to a unique token in the vocabulary that is indepen-\\ndent of values in the other columns. This means that in the\\nillustrated numerical transformation shown above, the “1”\\nin the ﬁrst column will have a different token id than the “1”\\npresent in the third column.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 3. Summary of the datasets used in the experiments for non-relational tabular data.\\nAbbr Name # Train # Validation # Test # Num # Cat Task type\\nAB Abalone 2672 669 836 7 1 Regression\\nAD Adult ROC 26048 6513 16281 6 8 Binclass\\nBU Buddy 12053 3014 3767 4 5 Multiclass\\nCA California Housing 13209 3303 4128 8 0 Regression\\nDI Diabetes 491 123 154 8 0 Binclass\\nFB Facebook Comments V olume 157638 19722 19720 50 1 Regression\\nB. Datasets\\nB.1. Non-relational tabular data\\nWe used six real-world datasets to assess the performance of our proposed model for generating realistic and useful synthetic\\ntabular data. The datasets are diverse with respect to the types of variables—mix of numerical and categorical data types—as\\nwell as the number of variables in each dataset—ranging from 8 to 51 columns. The collection includes, Abalone (OpenML)2,\\nAdult (income estimation)3, Buddy (Kaggle)4, California Housing (real estate data)5, Diabetes (OpenML)6, and Facebook\\nComments7. Original source, copyright, and license information are available in the links in the footnote.\\nWe used the data splits by Kotelnikov et al. (2022) published in Tab-DDPM GitHub. Based on their pickled numpy data\\ndumps, we recreated the splits to create data frames that we can use for our experiments with REaLTabFormer and GReaT.\\nThe latter model expects contextual input from the column names.\\nWe also used the open-sourced optimized model parameters published in the above GitHub repo after reviewing the code,\\nand the correctness of the code relevant to producing the assets of interest has been conﬁrmed. We trained the TV AE,\\nCTABGAN+, and Tab-DDPM models from scratch using the parameters on each dataset.\\nB.2. Relational tabular data\\nTo test the REaLTabFormer in modeling relational datasets, we used two real-world data: the Rossmann store sales8dataset\\nand the Airbnb new user bookings9dataset.\\nWe created train and test splits. For the Rossmann dataset, we used 80% of the stores data and their associated sales records\\nfor our training data. We used the remaining stores as the test data. We also limit the data used in the experiments from\\n2015-06 onwards spanning 2 months of sales data per store. In the Airbnb dataset, we considered a random sample of\\n10,000 users for the experiment. We take 8,000 as part of our training data, and we assessed the metrics and plots using the\\n2,000 users in the test data. We also limit the users considered to those having at most 50 sessions in the data.\\nC. Reproducibility\\nWe used begreat==0.0.3 for the GReaT model. We used the Tab-DDPM GitHub repo version with this permanent\\nlink https://github.com/rotot0/tab-ddpm/tree/41f2415a378f1e8e8f4f5c3b8736521c0d47cf22. We used sdv==0.17.2\\nandsdmetrics==0.8.1 ; however, we ﬁxed a bug in the HyperTransformer implementation. We used\\ntransformers==4.25.1 andtorch==1.13.1 . We will open-source the REaLTabFormer package and experi-\\nments repository. We used Python version 3.9 .\\nWe ran our experiments on a standalone workstation with the following specs: 2x AMD EPYC 7H12 64-Core Processor, 2x\\nRTX 3090 GPU, and 1TB RAM running Ubuntu 20.04 LTS.\\n2Abalone (OpenML)\\n3Adult (income estimation)\\n4Buddy (Kaggle)\\n5California Housing (real estate data)\\n6Diabetes (OpenML)\\n7Facebook Comments\\n8Rossmann store sales\\n9Airbnb new user bookings'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 13}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 4. Logistic detection measure for the generated parent, child, and merged tables by the Hierarchical Modeling Algorithm (HMA)\\nfrom SDV and the REaLTabFormer (RTF) models. This uses the logistic regression model as the detector.\\nDATASET TABLE SDV RTF\\nROSSMANNPARENT 78.67 ±6.79 92.75 ±4.28\\nCHILD 16.62 ±0.86 59.00 ±2.92\\nMERGED 12.00 ±0.73 50.69 ±2.41\\nAIRBNBPARENT 98.66 ±1.34 99.68 ±0.38\\nCHILD 0.00±0.00 26.33 ±0.78\\nMERGED 96.71 ±2.79 98.93 ±0.82\\nD. Other measures and results\\nWe also computed the logistic detection measure with the standard approach of using a logistic regression model. We ﬁnd\\nthat the logistic regression model appears to not provide reliable results Table 4. In particular, the scores returned by the\\nmodel are too high which is suspicious given that qualitative observation of the synthetic data hints at inaccuracies by both\\nmodels in producing perfect alignment with the original data. These spurious results may be due to the model’s limited\\ncapacity of learning the structure of the data. While techniques can be applied to help the model detect non-linearities better,\\nwe opted to report the results using the random forest as the base detector since it naturally is able to learn non-linearities\\nand appears to give reasonable results.\\nD.1. Joint plots\\nThe joint plot provides a qualitative assessment of the quality of the synthetic data generated by each model. We show in the\\nsequence of ﬁgures below the joint plots of two numerical variables in the datasets used.\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterOriginal\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterTVAE\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterCTABGAN+\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterT ab-DDPM\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterGReaT\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterREaLT abFormer\\nFigure 5. Joint plot of two numerical variables in the Abalone data compared across the samples generated by the different models.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 14}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numOriginal\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numTVAE\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numCTABGAN+\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numT ab-DDPM\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numGReaT\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numREaLT abFormer\\nFigure 6. Joint plot of two numerical variables in the Adult data compared across the samples generated by the different models.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)Original\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)TVAE\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)CTABGAN+\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)T ab-DDPM\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)GReaT\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)REaLT abFormer\\nFigure 7. Joint plot of two numerical variables in the Buddy data compared across the samples generated by the different models.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 15}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeOriginal\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeTVAE\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeCTABGAN+\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeT ab-DDPM\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeGReaT\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeREaLT abFormer\\nFigure 8. Joint plot of two numerical variables in the California housing data compared across the samples generated by the different\\nmodels.\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseOriginal\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseTVAE\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseCTABGAN+\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseT ab-DDPM\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseGReaT\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseREaLT abFormer\\nFigure 9. Joint plot of two numerical variables in the Diabetes data compared across the samples generated by the different models.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nD.2. Distance to closest record (DCR) distribution\\nWe present earlier the distance to closest record (DCR) distribution, Equation 2, as part of our proposed strategy to detect\\noverﬁtting in training the REaLTabFormer model. Here, we use the DCR distribution to visually assess whether the\\ngenerative models create exact copies of observations from the training data. We also show the DCR distribution of the real\\ntest data as a reference.\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityOriginal\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityTVAE\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityCTABGAN+\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityT ab-DDPM\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityGReaT\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityREaLT abFormer\\nFigure 10. Distance to closest record (DCR) distributions of the different models for the Abalone data.\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityOriginal\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityTVAE\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityCTABGAN+\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityT ab-DDPM\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityGReaT\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityREaLT abFormer\\nFigure 11. Distance to closest record (DCR) distributions of the different models for the Adult data.\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityOriginal\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityTVAE\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityCTABGAN+\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityT ab-DDPM\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityGReaT\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityREaLT abFormer\\nFigure 12. Distance to closest record (DCR) distributions of the different models for the Buddy data.\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityOriginal\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityTVAE\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityCTABGAN+\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityT ab-DDPM\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityGReaT\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityREaLT abFormer\\nFigure 13. Distance to closest record (DCR) distributions of the different models for the California housing data.\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityOriginal\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityTVAE\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityCTABGAN+\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityT ab-DDPM\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityGReaT\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityREaLT abFormer\\nFigure 14. Distance to closest record (DCR) distributions of the different models for the Diabetes data.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"RealTabformer.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursively splitting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='REaLTabFormer: Generating Realistic Relational\\nand Tabular Data using Transformers\\nAivin V . Solatorio1Olivier Dupriez1\\nAbstract\\nTabular data is a common form of organizing data.\\nMultiple models are available to generate syn-\\nthetic tabular datasets where observations are in-\\ndependent, but few have the ability to produce\\nrelational datasets. Modeling relational data is\\nchallenging as it requires modeling both a “par-\\nent” table and its relationships across tables. We'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='introduce REaLTabFormer (Realistic Relational\\nand Tabular Transformer), a tabular and relational\\nsynthetic data generation model. It ﬁrst creates a\\nparent table using an autoregressive GPT-2 model,\\nthen generates the relational dataset conditioned\\non the parent table using a sequence-to-sequence\\n(Seq2Seq) model. We implement target masking\\nto prevent data copying and propose the Qδstatis-\\ntic and statistical bootstrapping to detect over-\\nﬁtting. Experiments using real-world datasets'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='ﬁtting. Experiments using real-world datasets\\nshow that REaLTabFormer captures the relational\\nstructure better than a baseline model. REaLTab-\\nFormer also achieves state-of-the-art results on\\nprediction tasks, “out-of-the-box”, for large non-\\nrelational datasets without needing ﬁne-tuning.\\n1. Introduction\\nTabular data is one of the most common forms of data. Many\\ndatasets from surveys, censuses, and administrative sources\\nare provided in this form. These datasets may contain sensi-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='tive information that cannot be shared openly (Abdelhameed\\net al., 2018). Even when statistical disclosure methods are\\napplied, they may remain vulnerable to malicious attacks\\n(Cheng et al., 2017). As a result, their dissemination is\\nrestricted and the data have limited utility (O’Keefe & Ru-\\nbin, 2015). Differential privacy methods (Ji et al., 2014),\\nhomomorphic encryption approaches (Aslett et al., 2015;\\nWood et al., 2020), or federated machine learning (Yang'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='et al., 2019; Lin et al., 2021) may be implemented, allowing\\n1Development Economics Data Group, The World Bank, USA.\\nCorrespondence to:\\nAivin V . Solatorio <asolatorio@worldbank.org >\\nGitHub: @avsolatorio.\\nFigure 1. Illustration of the REaLTabFormer model. The left block\\nshows the non-relational tabular data model using GPT-2 with a\\ncausal LM head. In contrast, the right block shows how a relational\\ndataset’s child table is modeled using a sequence-to-sequence'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='(Seq2Seq) model. The Seq2Seq model uses the observations in\\nthe parent table to condition the generation of the observations in\\nthe child table. The trained GPT-2 model on the parent table, with\\nweights frozen, is also used as the encoder in the Seq2Seq model.\\ninsights from sensitive data to be accessible to researchers.\\nSynthetic tabular data with similar statistical properties as\\nthe real data offer an alternative, offering more value espe-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='cially in granular and segmentation analyses. To comply\\nwith data privacy requirements, the generative models that\\nproduce these synthetic data must provide guarantees that\\n“data copying” does not happen (Meehan et al., 2020; Car-\\nlini et al., 2023).\\nFormally, tabular data is a collection of observations (rows)\\noithat may or may not be independent. A single obser-\\nvation in a tabular data Twithncolumns is deﬁned by\\noi= [xi1,xi2,...,xij,...,xin], andjindicating the jthcol-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 0}, page_content='umn. We refer to tabular data having observations indepen-\\ndent of each other as non-relational tabular data . Tabular\\ndata having observations related to each other are referred to\\nasrelational tabular data . Relational datasets have at least\\none pair of tabular data ﬁles with a one-to-many mapping\\nof observations between the parent table and the child table,arXiv:2302.02041v1  [cs.LG]  4 Feb 2023'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nrespectively, linked by a unique identiﬁer. In the context of a\\nrelational dataset, a parent table is a non-relational tabular\\ndata, whereas the child table is a relational tabular data.\\nRelational tabular databases model the logical partitioning\\nof data and prevent unnecessary duplication of observations\\nfrom the parent to child tables (Jatana et al., 2012). Despite'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='its ubiquity, limited work has been done in generating syn-\\nthetic relational datasets. This may be due to the challenging\\nnature of modeling the complex relationships within and\\nacross tables.\\nThe ﬁeld of synthetic data generation has seen signiﬁcant\\ndevelopment in recent years (Gupta et al., 2016; Abufadda\\n& Mansour, 2021; Hernandez et al., 2022; Figueira & Vaz,\\n2022). Generative models have become mainstream with the\\nadvent of synthetic image generation models such as DALL-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='E (Ramesh et al., 2021), and most recently, ChatGPT. While\\ngenerative models for images and text are common, mod-\\nels for producing synthetic tabular data are comparatively\\nlimited despite their multiple possible applications. Syn-\\nthetic tabular data can contribute to addressing data privacy\\nissues and data sparseness (Appenzeller et al., 2022). They\\ncan help to make sensitive data accessible to researchers\\n(Goncalves et al., 2020), and to ﬁll gaps in data availabil-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='ity for counterfactual research and agent-based simulations\\n(Fagiolo et al., 2019), and for synthetic control methods\\n(Abadie et al., 2015). Further value can be derived from\\ntabular data by building predictive models using machine\\nlearning (Shwartz-Ziv & Armon, 2022). These predictive\\nmodels can infer variables of interest in the data that may\\notherwise be expensive to collect or correspond to some\\nsuccess metrics that can guide business decisions. Synthetic'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='data produced by deep learning models have been shown to\\nperform well in predictive modeling tasks. This extends the\\nutility of real-world data that may otherwise be unused due\\nto privacy concerns.\\nThis paper introduces the REalTabFormer , a transformer-\\nbased framework for generating non-relational tabular data\\nand relational datasets. It makes the following contributions:\\nUniﬁed framework The REalTabFormer uses an autore-\\ngressive (GPT-2) transformer to model non-relational tab-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='ular data for modeling and generating parent tables. It\\nthen models and generates observations in the child ta-\\nble using the sequence-to-sequence (Seq2Seq) (Yun et al.,\\n2019) framework. The encoder network uses the pre-trained\\nweights of the network for the parent table, contextualizing\\nthe input for generating arbitrary-length data corresponding\\nto observations in a child table, via the decoder network.\\nStrategies for privacy-preserving training Synthetic'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='data generation models must not only be able to generate\\nrealistic data but also implement safeguards to prevent the\\nmodel from “memorizing” and copying observations in the\\ntraining data during sampling. We use the distance to clos-est record (DCR), a data-copying measure, and statistical\\nbootstrapping to detect overﬁtting during training robustly.\\nWe introduce target masking for regularization to reduce the\\nlikelihood of training data being replicated by the model.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='Open-sourced models We publish the REaLTabFormer\\nmodels as an open-sourced Python package. Install the\\npackage using: pip install realtabformer .1\\nComprehensive evaluation We evaluate the performance\\nof our models on a variety of real-world datasets. We\\nuse open-sourced state-of-the-art models as baselines to\\nassess the performance of REaLTabFormer in generating\\nnon-relational and relational tabular datasets.\\nOur experiments demonstrate the effectiveness of the RE-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='aLTabFormer model for non-relational tabular data, beating\\ncurrent state-of-the-art in machine learning tasks for large\\ndatasets. We further demonstrate that the synthesized obser-\\nvations for the child table generated by the REaLTabFormer\\ncapture relational statistics more accurately than the baseline\\nmodels.\\n2. Related Work\\nRecent advances in deep learning, such as generative adver-\\nsarial networks (Park et al., 2018; Xu et al., 2019; Zhao et al.,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='2021), autoencoders (Li et al., 2019; Xu et al., 2019; Darabi\\n& Elor, 2021), language models (Borisov et al., 2022), and\\ndiffusion models (Kotelnikov et al., 2022) have been applied\\nto synthetic non-relational tabular data generation. These\\npapers demonstrate deep learning models’ capacity to pro-\\nduce more realistic data than traditional approaches such as\\nBayesian networks (Xu et al., 2019).\\nOn the other hand, generative models for relational datasets'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='are limited (Patki et al., 2016; Gueye et al., 2022). Exist-\\ning models are based on Hierarchical Modeling Algorithms\\n(Patki et al., 2016) where traditional statistical models, Gaus-\\nsian Copulas, are used to learn the joint distributions for\\neach and across tables. While these models can synthesize\\ndata, the quality of the generated data does not accurately\\ncapture the nuanced conditions within and across tables\\n(Fig. 2 and Fig. 3).\\nPadhi et al. (2021) presented TabGPT for generating syn-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 1}, page_content='thetic transactional data. They showed that autoregressive\\ntransformers, particularly GPT, can synthesize arbitrary-\\nlength data. One limitation of TabGPT is that one has\\nto train independent models to produce transactions for\\neach user. This becomes impractical for real-world applica-\\ntions. Our work generalizes the use of GPT by proposing a\\nsequence-to-sequence framework for generating arbitrary-\\nlength synthetic data conditioned on an input.\\n1https://github.com/avsolatorio/\\nREaLTabFormer'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n3. REaLTabFormer\\nREaLTabFormer is a transformer-based framework for gen-\\nerating non-relational tabular data using an autoregres-\\nsive model and relational tabular data using a sequence-\\nto-sequence (Seq2Seq) architecture. The framework also\\nconsists of strategies for encoding tabular data (Section 3.2),\\na statistical method to detect overﬁtting (Section 3.3.2), and'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='a constrained sampling strategy during generation.\\nDetails of the framework are described in this section. First,\\nwe present our proposed models to synthesize realistic re-\\nlational datasets. Next, we outline and describe the data\\nprocessing applied to the tabular data as input for train-\\ning the models. We then discuss solutions to improve our\\nmodel’s training and sampling process.\\n3.1. The REaLTabFormer Models\\nParent table model To generate synthetic observations'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='for a non-relational tabular data T, we model the conditional\\ndistribution of columnar values in each row of the data. Con-\\nsider a single observation oi= [xi1,xi2,...,xij,...,xin]in\\nTas deﬁned earlier. We treat oias a sequence with potential\\ndependencies across values xij, similar to a sentence in a\\ntext. This re-framing provides us with a framework to learn\\nthe conditional distribution xij∼P(X|xi1,xi2,...,xij−1)\\nand sequentially generate the next values in the sequence,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='eventually generating the full observation (Jelinek, 1985;\\nBengio et al., 2000). We use an autoregressive model to\\nlearn this distribution, Fig. 1. In the context of relational\\ndatasets, we use this approach to generate synthetic obser-\\nvations for the parent table T0. We extend this formulation\\nto generate the child table T′—a relational tabular data—\\nassociated with the parent table T0.\\nChild table model The extension is established by in-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='troducing a context learned by an encoder network from\\nobservations in T0. Instead of generating oiinT′indepen-\\ndently, we concatenate the child table observations related to\\nthe same observation in T0. This forms an arbitrary-length\\nsequencesi= [o1\\ni,o2\\ni,...,on\\ni], wherenis the number of\\nrelated observations in T′.\\nWe propose to model the generation of sigiven an obser-\\nvation in T0asxn\\nij∼P(X|o1\\ni,...,xn\\ni1,xn\\ni2,...,xn\\nij−1,Ck),\\nwhereCkis a context captured from a related observation'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='in the parent tabular data T0. We also use the same network\\ntrained on the parent table, with weights frozen , as the\\nSeq2Seq model’s encoder. This choice is expected to speed\\nup the training process since only the cross-attention layer\\nand the decoder network are needed to be trained for the\\nchild table model. The encoder network is assumed to have\\nlearned the properties of the parent table and will transfer\\nthis information to the decoder without further ﬁne-tuning\\nits weights, Fig 1.\\nJun'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='its weights, Fig 1.\\nJun\\n2015Jul08 15 22 29 06 13 20 27\\nDate0200040006000800010000Average salesOriginal\\nREaLT abFormer\\nSDVFigure 2. Graph of the daily mean of the Sales variable computed\\nfrom the original Rossmann dataset (blue), synthetic data produced\\nby REaLTabFormer (orange), and data generated by SDV (green).\\nThe REaLTabFormer closely captures the seasonality in the data\\ncompared with the HMA model from the SDV .\\nGPT-2: an autoregressive transformer Previous works'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='have shown that transformer-based autoregressive models\\ncan capture the conditional distribution of sequential data\\nvery well (Radford et al., 2019; Padhi et al., 2021). RE-\\naLTabFormer uses the GPT-2 architecture—a transformer-\\ndecoder architecture designed for autoregressive tasks—as\\nits base model. We adopt the same architecture for all GPT-2\\ninstances in the framework for simplicity. The GPT-2 archi-\\ntecture used in the REaLTabFormer has 768-dimensional'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='embeddings, 6 decoder layers, and 12 attention heads—a\\nset of parameters similar to DistilGPT2. We use the imple-\\nmentation from the HuggingFace transformers library (Wolf\\net al., 2020).\\n3.2. Tabular Data Encoding\\nThe GReaT model that uses pretrained large language mod-\\nels (LLMs) proposed by Borisov et al. (2022) offers insight\\ninto the minimal data processing requirements for language\\nmodels in generating tabular data. There is, however, the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='potential for optimization in using autoregressive language\\nmodels for this task, as the ﬁne-tuning process of a large\\npretrained model incurs computational costs. Particularly,\\nLLMs are trained on a large vocabulary where most of the\\ntokens are not needed for generating the tabular data at\\nhand. These unnecessary tokens increase the model’s com-\\nputational requirements and prolong training and sampling\\ntimes. To improve the efﬁciency of our model, we adopt'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 2}, page_content='a ﬁxed-set vocabulary as initially proposed by Padhi et al.\\n(2021). Generating a ﬁxed vocabulary for each column in\\nthe tabular data offers various advantages in training perfor-\\nmance and sampling. One of the main advantages is being'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n20-30 30-40 40-50 50-60 60-70 70-80 80-90 90+\\nage_group-unknown-\\nAndroid App ...\\nAndroid Phone\\nBlackberry\\nChromebook\\nLinux Desktop\\nMac Desktop\\nT ablet\\nWindows Desktop\\n_rtf_other_\\niPad T ablet\\niPhone\\niPodtouchdevice_typeOriginal\\n20-30 30-40 40-50 50-60 60-70 70-80 80-90 90+\\nage_groupSDV\\n20-30 30-40 40-50 50-60 60-70 70-80 80-90 90+\\nage_groupREaLT abFormer\\n0.00.20.40.60.81.0'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='age_groupREaLT abFormer\\n0.00.20.40.60.81.0\\nFigure 3. Joint distributions of the agegroup variable in the parent table and the device type in the child table of the Airbnb test\\ndataset (left), the SDV (middle), and the REalTabFormer (right). The plots show that the REaLTabFormer can synthesize values across the\\ndomain of the variables, while SDV learned only two device types out of thirteen. The REaLTabFormer also generalized and imputed age'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='values for users with “iPodtouch” device (red box). This device type group has missing values for age in the original data.\\nable to ﬁlter irrelevant tokens when generating values for\\na speciﬁc column. This directly contributes to efﬁciency\\nin sampling by reducing the chances of generating invalid\\nsamples. Our model performs minimal transformation of\\nthe raw data. First, we identify the various data types for\\neach column in the data. We then perform a series of data'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='processing speciﬁc to the column and data type. Notably,\\nwe adopt a fully text-based strategy in handling numeri-\\ncal values. These transformations produce a transformed\\ntabular data used to train the model. Borisov et al. (2022)\\nshowed that variable order has an insigniﬁcant impact on\\nlanguage models, so we did not apply variable permutation.\\nWe discuss the processing for each data type in Appendix A.\\nTraining data for the parent table The GPT-2 model we'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='use requires a set of token ids as input. To generate these\\nsequences of token ids, we ﬁrst create a vocabulary. This\\nvocabulary maps the unique tokens in each column to a\\nunique token id. Then, for each row in the modiﬁed data,\\nwe apply the mapping in the vocabulary to the tokens. This\\nproduces a list of token ids for each row of the data. The\\nmodel is then trained on an autoregressive task wherein the\\ntarget data corresponds to the right-shifted tokens of the\\ninput data.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='input data.\\nTraining data for the child table We concatenate the\\ntransformed observations corresponding to related rows in\\nthe child table. A special token is added before and after\\nthe set of tokens representing an individual observation.\\nIn this form, the data we use to train the Seq2Seq model\\ncontains input-output pairs. An input value contains a ﬁxed-\\nlength array of token ids representing the observation in\\nthe parent table. The input is similar to the input used'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='in the parent table model. An arbitrary-length array with\\nthe concatenated token ids for each related observation in\\nthe child table represents the output value. The number\\nof related observations that can be modeled is limited by\\ncomputational resources.3.3. REaLTabFormer Training and Sampling\\nDeep learning models for generative tasks face challenges of\\noverﬁtting the data resulting in issues such as data-copying\\n(Meehan et al., 2020; Carlini et al., 2023). Furthermore,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='observations generated by generative models for tabular\\ndata could face issues of validity and inconsistency. These\\nissues in the generated samples impact the efﬁciency of the\\ngenerative process. Our proposed framework addresses the\\naforementioned issues by, (i) introducing a robust statistical\\nmethod to monitor overﬁtting, and (ii) target masking to fur-\\nther reduce the risk of data copying. To improve the rate of\\nproducing valid observations by the model, we also imple-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='ment a constrained generation strategy during the sampling\\nstage.\\n3.3.1. T ARGET MASKING\\nData copying is a critical issue for deep learning-based gen-\\nerative tabular models as it can expose and compromise\\nsensitive information in the training data. To mitigate data-\\ncopying, we introduce target masking . Target masking is a\\nform of regularization aimed at minimizing the likelihood of\\nrecords in the training data being “memorized” and copied'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='by the generative model. Unlike the token masking intro-\\nduced in BERT (Devlin et al., 2018), where input tokens\\nare masked and the model is expected to predict the correct\\ntoken, target masking implements random replacement of\\nthe target or label tokens with a special mask token. This\\nartiﬁcially introduces missing values in the data.\\nWe intend for the model to learn the masks instead of the ac-\\ntual values. During the sampling stage, we then restrict the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 3}, page_content='mask token generation, forcing the model to ﬁll the value\\nwith a valid token probabilistically. Notably, even when the\\nmodel learns to copy the input-output pair, the learned out-\\nput corresponds to the masked version of the input. There-\\nfore, when we process the output, the probabilistic nature of\\nreplacing the mask token reduces the likelihood of generat-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\ning the training data. The mask rate parameter controls the\\nproportion of the tokens that are masked. We use a mask\\nrate of 10% in our experiments.\\n3.3.2. O VERFITTING ASSESSMENT\\nApplying deep learning models to small datasets may easily\\nresult in overﬁtting. This may cause privacy-related issues\\nwhen the model generates observations that are copied from\\nthe training data. Knowing when the model overﬁts the data'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='is also crucial when the purpose is to generate diverse out-of-\\nsample data. An overﬁtted model tends to generate samples\\ngenerally closer to the training data, thereby limiting the\\ngeneralization capacity of the model. While the former issue\\ncan be resolved by post-generation ﬁltering, the latter must\\nbe detected during the model training.\\nTaking hold-out data to detect overﬁtting is a common strat-\\negy in machine learning. Unfortunately, this strategy could'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='result in the premature termination of model training. It\\nmay also penalize a model based only on a small subset\\nof the data (Blum et al., 1999). The training procedures of\\nexisting state-of-the-art models do not explicitly check for\\noverﬁtting (Xu et al., 2019; Borisov et al., 2022; Kotelnikov\\net al., 2022). We propose and describe below an empiri-\\ncal statistical method to inform the generative model when\\noverﬁtting happens. The method allows for the full data to'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='be used in the training without the need for a hold-out set.\\nThe design of the method is expected to also help prevent\\ndata copying and the production of data that is riskily close\\nto the training data.\\nDistance to closest record We use the distance to the\\nclosest record (DCR) (Park et al., 2018) to measure the\\nsimilarity of synthetic samples to the original data. The\\nDCR is evaluated by taking a speciﬁed distance metric L\\nbetween the training data Trand the generated data G. We'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='then ﬁnd the smallest distance for each record. Consider the\\ndistance matrix between TrandG,\\nD=L(Tr,G) (1)\\nThe minimum value in each row iofDis the minimum\\ndistance of the ithrecord in the training data with respect\\nto all records in the generated data. We denote this set of\\nminimum values as ⃗di. The minimum value in each column\\njofDis the minimum distance of the jthrecord in the\\ngenerated data with respect to all records in the training'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='data. We denote this set of minimum values as ⃗dj. We then\\ntake⃗dg= [⃗di,⃗dj]as the distribution of distances to closest\\nrecords between the training data and the generated data.\\nWe deﬁne the quantity\\n⃗d= [⃗di,⃗dj] (2)\\nas the distance to closest record distribution for some Trandsome arbitrary sample. We also derive the DCR between\\nthe train dataset and some hold-out data Th. Let us denote\\nthis distribution of distances as ⃗dh. We use the distributions'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='⃗dgand⃗dhin our proposed non-parametric method.\\nQuantile difference ( Qδ) statistic Two samples from a\\nsimilar distribution should, on average, have approximately\\nthe same values at each quantile of the distribution. To detect\\nwhether two samples come from different distributions, we\\ndeﬁne a set of quantiles over which we compare the two\\nsamples. For each quantile in the set, we ﬁnd the value at\\nthe given quantile in one sample and measure the proportion'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='of the values in the other sample that are below it. If the\\ndistributions are similar, the proportion should be close to\\nthe given quantile, for all quantiles being tested.\\nFormally, let ShandSghavingmandnobservations, re-\\nspectively, be two samples being compared. Let Qbe a\\nset ofNquantiles, and q∈Qis a speciﬁc quantile in the\\nset. Consider vqas the value in Shat quantileq. Then, we\\ncompute the value\\npq=∑\\nx∈Sg[x≤vq]\\nn(3)\\nwherepqis the proportion of values in Sgthat are less than'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='or equal tovq. We deﬁne a statistic\\nQδ=1\\nN∑\\nq(pq−q) (4)\\nThis formulation has similarities with the Cramer-von Mises\\nω2criterion, but the Qδstatistic has one key difference: the\\nasymmetry of the statistic. This stems from the fact that\\nthe choice of which sample is considered as Sh—the dis-\\ntribution from which vqis identiﬁed—matters. Since we\\nare averaging over the quantiles, this statistic may not yield\\nconclusive guidance for distributions with cumulative dis-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='tribution functions (CDFs) intersecting at some quantile.\\nNonetheless, this statistic works best in detecting the dissim-\\nilarity of the two samples at the left tail of the distribution\\nwhich matters most for our purpose. This is because the\\ndistributions we are comparing are the DCRs. We want\\nto detect when the distance between the sample and the\\ntraining data is signiﬁcantly closer to zero than expected.\\nWe use theQδstatistic as the basis for detecting overﬁtting.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 4}, page_content='The threshold against which this statistic will be compared\\nduring training is produced through an empirical bootstrap-\\nping over random samples from the training data. The\\ndetails of the bootstrapping method are explained next.\\nQδstatistic threshold via bootstrapping We use three\\nhyperparameters in estimating the threshold that will signal\\nwhen overﬁtting occurs during training. First, a sample pro-\\nportionρcorresponds to a fraction of the training data. This'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 1. Machine learning efﬁcacy (MLE) and discriminator measure (DM) evaluated on the synthetic data generated by the models\\n(columns) trained on six real-world datasets (rows): Abalone (AB), Adult income (AD), Buddy (BU), California housing (CA), Diabetes\\n(DI), and Facebook Comments (FB). The MLE is measured by the R2for regression, while macro average F1is used for classiﬁcation'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='tasks; higher scores are better. A discriminator measure closer to 50% is better. Best scores are highlighted for the MLE measure,\\nconsidering standard deviation. No reported results for GReaT on the FB dataset due to impractical training time.\\nOriginal TV AE CTABGAN+ Tab-DDPM GReaT REaLTabFormer\\nAB (R2)MLE ( ↑) 0.5562 ±0.004 0.3943 ±0.012 0.4697 ±0.014 0.5248 ±0.011 0.3530 ±0.031 0.5035 ±0.011\\nDM ( ↓) - 82.96 ±2.42 75.64±1.20 59.88±2.22 70.46±0.92 63.08±1.18'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='AD (F1)MLE 0.8155 ±0.001 0.7695 ±0.004 0.7778 ±0.003 0.7922 ±0.002 0.7997 ±0.002 0.8113 ±0.002\\nDM - 95.48 ±1.34 61.17±0.40 53.73±0.22 68.04±0.26 55.78±0.20\\nBU (F1)MLE 0.9303 ±0.002 0.9233 ±0.002 0.9267 ±0.002 0.9057 ±0.003 0.9279 ±0.003 0.9278 ±0.003\\nDM - 66.56 ±0.56 58.33±0.49 54.43±0.47 62.18±0.45 55.86±0.47\\nCA (R2)MLE 0.8568 ±0.001 0.7373 ±0.004 0.5231 ±0.006 0.8252 ±0.002 0.7189 ±0.004 0.8076 ±0.003\\nDM - 62.06 ±0.60 90.14±1.03 54.30±0.89 66.78±0.47 57.29±0.56'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='DI (F1)MLE 0.7759 ±0.014 0.7395 ±0.035 0.7339 ±0.024 0.7448 ±0.031 0.7419 ±0.03 0.7315 ±0.027\\nDM - 90.16 ±1.31 70.94±1.99 69.00±1.56 74.88±1.79 75.56±2.84\\nFB (R2)MLE 0.8371 ±0.001 0.6374 ±0.007 0.4722 ±0.053 0.6850 ±0.006 - 0.7702 ±0.004\\nDM - 97.72 ±0.80 93.60±0.28 66.07±0.23 - 65.46 ±0.83\\nfraction will be randomly sampled during the bootstrapping\\nand evaluation phases of the generative model training. Sec-\\nond, theαvalue for choosing the critical threshold for the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='bootstrap statistic. Third, we specify a bootstrap round B\\ncorresponding to the number of times we compute the Qδ\\nstatistic between three random samples—two, each of size\\nρ, and the rest having size 1−2ρof the training data.\\nFormally, for a given training data TrwithNobservations,\\nwe deﬁne a bootstrap method to generate a conﬁdence in-\\nterval for the Qδstatistic speciﬁc to the tabular data at hand.\\nFor each bootstrap round b∈B, we take three random'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='samplesStr,Sh, andSg, without replacement. ShandSg\\nare each of size ρN, whileStrcontains (1−2ρ)Nsamples.\\nWe compute the DCR distributions ⃗dgand⃗dhfor the two\\nsamplesShandSg, respectively, relative to sample Str. We\\nthen compute the Qδstatistic between ⃗dgand⃗dh, where we\\ntake⃗dhas the distribution from which we compute the value\\nvqin Equation 3. We store the statistic computed across the\\nbootstrap rounds. We use the speciﬁed αto get the cutoff'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='value that will be used as the statistic threshold. We use\\nthis threshold Q′\\nδduring training to compare the Qδstatistic\\nderived from the generated samples by the model. We set\\nρ= 0.165,α= 0.95, andB= 500 in our experiments.\\nEarly stopping with Q′\\nδOur training procedure is paused\\nat each epoch that is a multiple of E. We generate data from\\nthe model during these epochs. The generated data has size\\nSg. We then take two mutually exclusive random samples'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='from the training data, without replacement, to represent\\nStrandSh. We compute the Qe\\nδfor this epoch based on the\\nsamples generated and drawn. Then, we compare this statis-\\ntic to the previously computed threshold Q′\\nδ. We continuetraining the model if Qe\\nδ< Q′\\nδ. We save a checkpoint of\\nthis model. We terminate the model training when Qe\\nδ>Q′\\nδ\\nforXconsecutive epochs. We then load the checkpoint for\\nthe most recent model that satisﬁed the condition Qe\\nδ<Q′\\nδ.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='δ<Q′\\nδ.\\nIn our experiments, we set E= 5as the period of our over-\\nﬁtting evaluation and X= 2 as our grace period before\\ntraining termination.\\n3.3.3. S AMPLING\\nThe models we use build each observation sequentially,\\none token at a time. We leverage the structure of our data\\nprocessing to optimize the generation of samples from the\\ntrained models. Using a vocabulary speciﬁc to a column in\\nthe input data allows us to implement a constrained genera-\\ntion of tokens for each column.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='tion of tokens for each column.\\nWe track the token ids that form the domain of each column\\nduring the generation of the vocabulary using a hash map.\\nBased on this, the tokens that are invalid for the columns will\\nnot be considered for generation in the timestep representing\\nthe column. This strategy allows for efﬁcient sampling\\nwherein the likelihood of generating an invalid sample is\\nclose to zero. In our experiments, ≪1%invalid samples\\nare generated during the sampling phase.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 5}, page_content='are generated during the sampling phase.\\n4. Experiments and Results\\nThis section outlines the evaluation process we conducted to\\nquantify the performance of the proposed REaLTabFormer\\nframework compared with baseline models. We ﬁrst demon-\\nstrate that the performance of the model we use to generate\\nthe parent tables, and non-relational tabular data in gen-\\neral, compares with or exceeds the performance of state-of-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 2. Logistic detection (LD) measure using random forest\\nmodel for the generated parent, child, and merged tables by the\\nHierarchical Modeling Algorithm (HMA) from SDV and the RE-\\naLTabFormer (RTF) models. Our model consistently beats the\\nSDV model on this metric.\\nDATASET TABLE SDV RTF\\nROSSMANNPARENT 31.77 ±3.41 81.04 ±4.54\\nCHILD 6.53±0.39 52.08 ±0.89\\nMERGED 2.80±0.25 28.33 ±2.31'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='MERGED 2.80±0.25 28.33 ±2.31\\nAIRBNBPARENT 7.37±0.72 89.65 ±1.92\\nCHILD 0.00±0.00 30.48 ±0.79\\nMERGED 0.00±0.00 21.43 ±1.10\\nthe-art models in real-world tabular data generation tasks\\nmeasured by the machine learning efﬁcacy metric. We also\\nuse the discriminator measure to quantify how realistic the\\nsamples generated by each model are. We proceed to model\\nreal-world relational datasets and show, quantitatively using\\nlogistic detection, that the synthetic data produced by the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='REaLTabFormer are more realistic and accurate.\\n4.1. Data\\nWe use a collection of real-world datasets, listed in Table 3,\\ncommonly used in previous works for non-relational tabular\\ndata generation (Xu et al., 2019; Zhao et al., 2021; Gor-\\nishniy et al., 2021; Borisov et al., 2022; Kotelnikov et al.,\\n2022). These datasets differ with respect to the number of\\nobservations, ranging from 768 up to 197,080 observations.\\nThere is also variation in the number of variables they con-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='tain, ranging from 8 to 50 numerical variables and 0 up to 8\\ncategorical variables. The datasets cover regression, binary,\\nand multi-class classiﬁcation prediction tasks.\\nWe use two real-world datasets to compare the performance\\nof the REaLTabFormer on modeling relational tabular data\\ncompared with the baseline. These datasets are the Ross-\\nmann dataset and the Airbnb dataset used in prior work on\\nsynthetic relation data generation (Patki et al., 2016).\\n4.2. Baseline models'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='4.2. Baseline models\\nNon-relational tabular data We use models that apply\\ndifferent deep learning architectures for generating non-\\nrelational tabular data as baselines to compare the REaLTab-\\nFormer model with. The TV AE is based on variational\\nautoencoder (Xu et al., 2019), the CTABGAN+ on GAN\\narchitecture (Zhao et al., 2022), the Tab-DDPM on diffusion\\n(Kotelnikov et al., 2022), and GReaT uses pretrained LLM\\n(Borisov et al., 2022).\\nRelational datasets Models for generating relational'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='datasets are limited. Gueye et al. (2022) published work on\\nusing GAN for relational datasets but no open-sourced im-\\na b c d\\nStoreType0200040006000800010000SalesData\\nOriginal\\nREaLT abFormer\\nSDVFigure 4. Summary of the average “Sales” variable in the child\\ntable of the Rossmann dataset grouped by “StoreType” variable in\\nthe parent table. The values shown are from the original data (blue),\\nsynthetic data produced by REaLTabFormer (orange), and data'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='generated by SDV (green). This graph shows that REaLTabFormer\\ncaptures the inter-table variations and relationships well.\\nplementation is available. We choose to limit our baselines\\nto open-sourced models; hence, we only use the Hierarchi-\\ncal Modeling Algorithm (HMA) available in the Synthetic\\nData Vault (SDV) as our baseline (Patki et al., 2016).\\n4.3. Generative models training\\nThe GReaT model was trained for 100 epochs for each\\ndata. The parameters for the TV AE, CTABGAN+, and'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='data. The parameters for the TV AE, CTABGAN+, and\\nTab-DDPM models had been tuned for the predictive task\\nitself using the real validation data from Kotelnikov et al.\\n(2022). For the relational datasets, we trained the HMA\\nmodel as prescribed in the SDV documentation. In contrast,\\nthe REaLTabFormer model was not tuned against any of\\nthe machine learning tasks. The model solely relied on the\\noverﬁtting metric discussed in Section 3.3.2. We used the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='same parameters for the different datasets to test how the\\nREaLTabFormer performs “out-of-the-box”.\\n4.4. Measures and Results\\nWe select the following measures to quantify the quality and\\nutility of the generated samples by the generative models.\\nMachine Learning (ML) efﬁcacy The machine learning\\n(ML) efﬁcacy (Xu et al., 2019; Kotelnikov et al., 2022;\\nBorisov et al., 2022) measures the potential utility of the\\nsynthetic data to supplant the real data for machine learn-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 6}, page_content='ing tasks, in particular, training a prediction model. The\\nML efﬁcacy reported by Borisov et al. (2022) in their work\\nused ML models that were not ﬁne-tuned. Kotelnikov et al.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n(2022) showed that the ML efﬁcacy computed from models\\nthat are not ﬁne-tuned may show spurious results. They in-\\nstead optimized the ML models—CatBoost (Prokhorenkova\\net al., 2018)—they used in reporting the ML efﬁcacy. This\\napproach is closer to what researchers are expected to do\\nin the real world, therefore, we adopt these tuned models\\nin our experiments. We generate a validation set from the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='generative models to signal the early-stopping condition\\nduring the ML model training. This is in contrast with the\\nmethod used by Kotelnikov et al. (2022) where they still\\nrelied on the real validation data for the early-stopping of\\nthe ML model.\\nWe report the macro average F1 score (Opitz & Burst, 2019)\\nfor classiﬁcation tasks and the R2metric for regression\\ntasks. Our results presented in Table 1 (MLE) show that\\nREaLTabFormer, despite not being ﬁne-tuned, produces ML'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='efﬁcacy scores that are the best or second-best compared\\nwith the baselines. This demonstrates that REaLTabFormer\\ncan be used, “out-of-the-box”, to generate synthetic data\\nwith state-of-the-art performance in machine learning tasks.\\nThe FB comments dataset, where REaLTabFormer obtained\\nthe best performance, is the largest dataset tested and has\\nthe largest number of columns. Training the GReaT model\\non this dataset yielded impractical runtime so no result is'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='reported. This supports our view that using LLM trained\\non a large vocabulary, containing a majority of irrelevant\\ntokens, limits the efﬁciency of the model.\\nDiscriminator measure We adopt the discriminator mea-\\nsure (Borisov et al., 2022) to quantify whether the data\\ngenerated by a model is easily distinguishable from real\\ndata. A dataset is made by combining an equal number of\\nreal and synthetic data. Real observations in this dataset are'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='labeled as “1” and synthetic observations are labeled as “0”.\\nSimilar to Borisov et al. (2022), we train a random forest\\nmodel to predict the labels given an observation. A held-out\\ndataset containing a combination of synthetic samples and\\nreal test data is then used to report the ﬁnal measure.\\nAn accuracy that is closer to 50% implies better synthetic\\ndata quality since the discriminative model is not able to\\ndistinguish the real from the synthetic observations. We'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='report our results in Table 1 (DM). The DM measure shows\\nthat the Tab-DDPM has the most indistinguishable synthetic\\ndata. Nonetheless, REaLTabFormer, without the need for\\ntuning, has DM measures that are close to the Tab-DDPM.\\nThis suggests that the synthetic data produced by a diffusion-\\nbased model and REaLTabFormer are realistic compared\\nwith the other baselines.\\nLogistic Detection For relational datasets, we use logistic\\ndetection (LD) (Fisher et al., 2019; Gueye et al., 2022) to'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='quantify the quality of the parent, child, and merged tables\\ngenerated by REaLTabFormer compared with the HMAmodel. We evaluate ROC-AUC scores averaged over (N=3)\\ncross-validation folds,\\nµRA=1\\nNN∑\\ni=1max(0.5,ROC−AUC)×2−1 (5)\\nThe value reported is LD= 100×(1−µRA), where scores\\nrange from 0 to 100, and scores closer to 100 imply better\\nsynthetic data quality. We use random forest in measuring\\nthe logistic detection instead of the standard logistic regres-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='sion model. The random forest model captures non-linearity\\nin the data well than logistic regression (Couronn ´e et al.,\\n2018), reducing the likelihood of spurious results. We report\\nthe results in Table 2. Additional LD results using logistic\\nregression are shown in Table 4.\\nThe REaLTabFormer model produces signiﬁcantly higher-\\nquality synthetic data than the HMA model across the\\ndatasets tested. The high values of LD for the child and\\nthe merged tables highlight the ability of REaLTabFormer'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='to accurately synthesize relational datasets in comparison\\nwith the leading baseline. The LD metric shows that data\\ngenerated by SDV for the Airbnb child table is entirely dis-\\ntinguishable from the real data. The quantitative results are\\nsupported by relational statistics computed from synthetic\\ndatasets produced by REaLTabFormer and the HMA model:\\nFigures 2 to 4.\\n5. Conclusion\\nWe presented REaLTabFormer, a framework capable of\\ngenerating high-quality non-relational tabular data and re-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='lational datasets. This work extends the application of\\nsequence-to-sequence models to modeling and generating\\nrelational datasets. We introduced target masking as a com-\\nponent in the model to mitigate data-copying and safeguard-\\ning from potentially sensitive data leaking from the training\\ndata. We proposed a statistical method and the Qδstatistic\\nfor detecting overﬁtting in model training. This statistical\\nmethod may be adapted to other generative model training.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='We showed that our proposed model generates realistic syn-\\nthetic tabular data that can be a proxy for real-world data in\\nmachine learning tasks. REaLTabFormer’s ability to model\\nrelational datasets accurately compared with existing open-\\nsourced alternative contributes to solving existing gaps in\\ngenerative models for realistic relational datasets. Finally,\\nthis work can be extended and applied to data imputation,\\ncross-survey imputation, and upsampling for machine learn-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 7}, page_content='ing with imbalanced data. A BERT-like encoder can be used\\ninstead of GPT-2 with the REaLTabFormer for modeling\\nrelational datasets. We also see opportunities to improve\\nprivacy protection strategies and the development of more\\ncomponents like target masking embedded into synthetic\\ndata generation models to prevent sensitive data exposure.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n6. REaLTabFormer Python Package\\nWe publish the REaLTabFormer as a package on PyPi. We\\nshow below how the model can be easily trained on any\\ntabular dataset, loaded as a Pandas DataFrame.\\n6.1. Non-relational tabular model\\nUse the following snippet to ﬁt the REaLTabFormer on a\\nnon-relational tabular dataset. One can control the various\\nhyper-parameters of the model and the ﬁtting method, e.g.,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='the number of bootstrap rounds numbootstrap , the frac-\\ntion of training data frac used to generate the Qδstatistic,\\netc. Keyword arguments for the HuggingFace transformers\\nTrainer class can also be passed as **kwargs when\\ninitializing the model.\\n1# pip install realtabformer\\n2import pandas as pd\\n3from realtabformer import REaLTabFormer\\n4\\n5# NOTE: Remove any unique identifiers in the\\n6# data that you don\\'t want to be modeled.\\n7df = pd.read_csv(\"foo.csv\")\\n8\\n9# Non-relational or parent table.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='8\\n9# Non-relational or parent table.\\n10rtf_model = REaLTabFormer(\\n11 model_type=\"tabular\",\\n12 gradient_accumulation_steps=4)\\n13\\n14# Fit the model on the dataset.\\n15# Additional parameters can be\\n16# passed to the /grave.ts1.fit /grave.ts1method.\\n17rtf_model.fit(df)\\n18\\n19# Save the model to the current directory.\\n20# A new directory /grave.ts1rtf_model/ /grave.ts1will be created.\\n21# In it, a directory with the model\\'s\\n22# experiment id /grave.ts1idXXXX /grave.ts1will also be created'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='23# where the artefacts of the model will be stored.\\n24rtf_model.save(\"rtf_model/\")\\n25\\n26# Generate synthetic data with the same\\n27# number of observations as the real dataset.\\n28samples = rtf_model.sample(n_samples=len(df))\\n29\\n30# Load the saved model. The directory to the\\n31# experiment must be provided.\\n32rtf_model2 = REaLTabFormer.load_from_dir(\\n33 path=\"rtf_model/idXXXX\")\\n6.2. Non-relational tabular model\\nREaLTabFormer for relational databases requires a two-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='phase training. First, the model for the parent table is trained\\nas a non-relational tabular data, then saved. Second, we\\npass the path of the saved parent model when creating the\\nREaLTabFormer instance for the child model to be used as\\nits encoder, then train. Generate synthetic samples from the\\nparent table and use as input to the trained child model to\\ngenerate the synthetic relational observations.1# pip install realtabformer\\n2import os\\n3import pandas as pd'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='2import os\\n3import pandas as pd\\n4from realtabformer import REaLTabFormer\\n5\\n6pdir = Path(\"rtf_parent/\")\\n7parent_df = pd.read_csv(\"foo.csv\")\\n8child_df = pd.read_csv(\"bar.csv\")\\n9join_on = \"unique_id\"\\n10\\n11# Make sure that the key columns in both the\\n12# parent and the child table have the same name.\\n13assert ((join_on inparent_df.columns) and\\n14 (join_on inchild_df.columns))\\n15\\n16# Non-relational or parent table. Don\\'t include the\\n17# unique_id field.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='17# unique_id field.\\n18parent_model = REaLTabFormer(model_type=\"tabular\")\\n19parent_model.fit(parent_df.drop(join_on, axis=1))\\n20parent_model.save(pdir)\\n21\\n22# # Get the most recently saved parent model,\\n23# # or a specify some other saved model.\\n24# parent_model_path = pdir / \"idXXX\"\\n25parent_model_path = sorted([\\n26 pfor pinpdir.glob(\"id *\")ifp.is_dir()],\\n27 key=os.path.getmtime)[-1]\\n28\\n29child_model = REaLTabFormer(\\n30 model_type=\"relational\",\\n31 parent_realtabformer_path=parent_model_path,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='31 parent_realtabformer_path=parent_model_path,\\n32 output_max_length= None , train_size=0.8)\\n33\\n34child_model.fit(\\n35 df=child_df, in_df=parent_df, join_on=join_on)\\n36\\n37# Generate parent samples.\\n38parent_samples = parent_model.sample(len(parend_df))\\n39\\n40# Create the unique ids based on the index.\\n41parent_samples.index.name = join_on\\n42parent_samples = parent_samples.reset_index()\\n43\\n44# Generate the relational observations.\\n45child_samples = child_model.sample('),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='45child_samples = child_model.sample(\\n46 input_unique_ids=parent_samples[join_on],\\n47 input_df=parent_samples.drop(join_on, axis=1),\\n48 gen_batch=64)\\nAcknowledgments This project was supported by the\\n“Enhancing Responsible Microdata Access to Improve\\nPolicy and Response in Forced Displacement Situations”\\nproject funded by the World Bank-UNHCR Joint Data Cen-\\nter on Forced Displacement (JDC) – KP-P174174-GINP-\\nTF0B5124. We also thank Patrick Brock for providing'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 8}, page_content='insightful comments. The ﬁndings, interpretations, and\\nconclusions expressed in this paper are entirely those of\\nthe authors. They do not necessarily represent the views\\nof the International Bank for Reconstruction and Develop-\\nment/World Bank and its afﬁliated organizations, or those\\nof the Executive Directors of the World Bank or the govern-\\nments they represent.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nReferences\\nAbadie, A., Diamond, A., and Hainmueller, J. Compara-\\ntive politics and the synthetic control method. American\\nJournal of Political Science , 59(2):495–510, 2015.\\nAbdelhameed, S. A., Moussa, S. M., and Khalifa, M. E.\\nPrivacy-preserving tabular data publishing: a comprehen-\\nsive evaluation from web to cloud. Computers & Security ,\\n72:74–95, 2018.\\nAbufadda, M. and Mansour, K. A survey of synthetic data'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='generation for machine learning. In 2021 22nd Inter-\\nnational Arab Conference on Information Technology\\n(ACIT) , pp. 1–7. IEEE, 2021.\\nAppenzeller, A., Leitner, M., Philipp, P., Krempel, E., and\\nBeyerer, J. Privacy and utility of private synthetic data for\\nmedical data analyses. Applied Sciences , 12(23):12320,\\n2022.\\nAslett, L. J., Esperan c ¸a, P. M., and Holmes, C. C. A re-\\nview of homomorphic encryption and software tools for\\nencrypted statistical machine learning. arXiv preprint'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='arXiv:1508.06574 , 2015.\\nBengio, Y ., Ducharme, R., and Vincent, P. A neural proba-\\nbilistic language model. Advances in neural information\\nprocessing systems , 13, 2000.\\nBlum, A., Kalai, A., and Langford, J. Beating the hold-out:\\nBounds for k-fold and progressive cross-validation. In\\nProceedings of the twelfth annual conference on Compu-\\ntational learning theory , pp. 203–208, 1999.\\nBorisov, V ., Seßler, K., Leemann, T., Pawelczyk, M., and\\nKasneci, G. Language Models are Realistic Tabular Data'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='Generators, October 2022. arXiv:2210.06280 [cs].\\nCarlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag,\\nV ., Tram `er, F., Balle, B., Ippolito, D., and Wallace, E.\\nExtracting training data from diffusion models. arXiv\\npreprint arXiv:2301.13188 , 2023.\\nCheng, L., Liu, F., and Yao, D. Enterprise data breach:\\ncauses, challenges, prevention, and future directions. Wi-\\nley Interdisciplinary Reviews: Data Mining and Knowl-\\nedge Discovery , 7(5):e1211, 2017.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='edge Discovery , 7(5):e1211, 2017.\\nCouronn ´e, R., Probst, P., and Boulesteix, A.-L. Random\\nforest versus logistic regression: a large-scale benchmark\\nexperiment. BMC bioinformatics , 19:1–14, 2018.\\nDarabi, S. and Elor, Y . Synthesising multi-modal\\nminority samples for tabular data. arXiv preprint\\narXiv:2105.08204 , 2021.Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805 ,\\n2018.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='2018.\\nFagiolo, G., Guerini, M., Lamperti, F., Moneta, A., and\\nRoventini, A. Validation of agent-based models in eco-\\nnomics and ﬁnance. In Computer simulation validation ,\\npp. 763–787. Springer, 2019.\\nFigueira, A. and Vaz, B. Survey on synthetic data generation,\\nevaluation methods and gans. Mathematics , 10(15):2733,\\n2022.\\nFisher, C. K., Smith, A. M., and Walsh, J. R. Machine\\nlearning for comprehensive forecasting of alzheimer’s\\ndisease progression. Scientiﬁc reports , 9(1):1–14, 2019.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='Goncalves, A., Ray, P., Soper, B., Stevens, J., Coyle, L.,\\nand Sales, A. P. Generation and evaluation of synthetic\\npatient data. BMC medical research methodology , 20(1):\\n1–40, 2020.\\nGorishniy, Y ., Rubachev, I., Khrulkov, V ., and Babenko,\\nA. Revisiting deep learning models for tabular data. Ad-\\nvances in Neural Information Processing Systems , 34:\\n18932–18943, 2021.\\nGueye, M., Attabi, Y ., and Dumas, M. Row conditional-\\ntgan for generating synthetic relational databases. arXiv'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='preprint arXiv:2211.07588 , 2022.\\nGupta, A., Vedaldi, A., and Zisserman, A. Synthetic data\\nfor text localisation in natural images. In Proceedings\\nof the IEEE conference on computer vision and pattern\\nrecognition , pp. 2315–2324, 2016.\\nHernandez, M., Epelde, G., Alberdi, A., Cilla, R., and\\nRankin, D. Synthetic data generation for tabular health\\nrecords: A systematic review. Neurocomputing , 2022.\\nJatana, N., Puri, S., Ahuja, M., Kathuria, I., and Gosain, D.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='A survey and comparison of relational and non-relational\\ndatabase. International Journal of Engineering Research\\n& Technology , 1(6):1–5, 2012.\\nJelinek, F. Markov source modeling of text generation. In\\nThe impact of processing techniques on communications ,\\npp. 569–591. Springer, 1985.\\nJi, Z., Lipton, Z. C., and Elkan, C. Differential privacy and\\nmachine learning: a survey and review. arXiv preprint\\narXiv:1412.7584 , 2014.\\nKotelnikov, A., Baranchuk, D., Rubachev, I., and Babenko,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 9}, page_content='A. Tabddpm: Modelling tabular data with diffusion mod-\\nels.arXiv preprint arXiv:2209.15421 , 2022.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nLi, S.-C., Tai, B.-C., and Huang, Y . Evaluating variational\\nautoencoder as a private data release mechanism for tab-\\nular data. In 2019 IEEE 24th Paciﬁc Rim International\\nSymposium on Dependable Computing (PRDC) , pp. 198–\\n1988. IEEE, 2019.\\nLin, J., Ma, J., and Zhu, J. Privacy-preserving house-\\nhold characteristic identiﬁcation with federated learning\\nmethod. IEEE Transactions on Smart Grid , 13(2):1088–'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='1099, 2021.\\nMeehan, C., Chaudhuri, K., and Dasgupta, S. A non-\\nparametric test to detect data-copying in generative mod-\\nels. In International Conference on Artiﬁcial Intelligence\\nand Statistics , 2020.\\nO’Keefe, C. M. and Rubin, D. B. Individual privacy versus\\npublic good: protecting conﬁdentiality in health research.\\nStatistics in medicine , 34(23):3081–3103, 2015.\\nOpitz, J. and Burst, S. Macro f1 and macro f1. arXiv\\npreprint arXiv:1911.03347 , 2019.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='preprint arXiv:1911.03347 , 2019.\\nPadhi, I., Schiff, Y ., Melnyk, I., Rigotti, M., Mroueh, Y .,\\nDognin, P., Ross, J., Nair, R., and Altman, E. Tabular\\nTransformers for Modeling Multivariate Time Series. In-\\nstitute of Electrical and Electronics Engineers Inc., June\\n2021. doi: 10.1109/ICASSP39728.2021.9414142. ISSN:\\n15206149.\\nPark, N., Mohammadi, M., Gorde, K., Jajodia, S., Park,\\nH., and Kim, Y . Data synthesis based on generative\\nadversarial networks. arXiv preprint arXiv:1806.03384 ,\\n2018.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='2018.\\nPatki, N., Wedge, R., and Veeramachaneni, K. The Synthetic\\nData Vault. In 2016 IEEE International Conference on\\nData Science and Advanced Analytics (DSAA) , pp. 399–\\n410, 2016. doi: 10.1109/DSAA.2016.49.\\nProkhorenkova, L., Gusev, G., V orobev, A., Dorogush, A. V .,\\nand Gulin, A. Catboost: unbiased boosting with categori-\\ncal features. Advances in neural information processing\\nsystems , 31, 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='Sutskever, I. Language Models are Unsupervised Multi-\\ntask Learners. 2019.\\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Rad-\\nford, A., Chen, M., and Sutskever, I. Zero-shot text-\\nto-image generation. In International Conference on\\nMachine Learning , pp. 8821–8831. PMLR, 2021.\\nShwartz-Ziv, R. and Armon, A. Tabular data: Deep learning\\nis not all you need. Information Fusion , 81:84–90, 2022.Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\\nY ., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M.,\\nLhoest, Q., and Rush, A. Transformers: State-of-the-Art\\nNatural Language Processing. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Language\\nProcessing: System Demonstrations , pp. 38–45, Online,\\nOctober 2020. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2020.emnlp-demos.6.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='doi: 10.18653/v1/2020.emnlp-demos.6.\\nWood, A., Najarian, K., and Kahrobaei, D. Homomorphic\\nencryption for machine learning in medicine and bioin-\\nformatics. ACM Computing Surveys (CSUR) , 53(4):1–35,\\n2020.\\nXu, L., Skoularidou, M., Cuesta-Infante, A., and Veera-\\nmachaneni, K. Modeling tabular data using conditional\\nGAN. In Proceedings of the 33rd International Confer-\\nence on Neural Information Processing Systems , number\\n659, pp. 7335–7345. Curran Associates Inc., Red Hook,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='NY , USA, December 2019.\\nYang, Q., Liu, Y ., Chen, T., and Tong, Y . Federated machine\\nlearning: Concept and applications. ACM Transactions\\non Intelligent Systems and Technology (TIST) , 10(2):1–19,\\n2019.\\nYun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and\\nKumar, S. Are transformers universal approximators\\nof sequence-to-sequence functions? arXiv preprint\\narXiv:1912.10077 , 2019.\\nZhao, Z., Kunar, A., Birke, R., and Chen, L. Y . Ctab-gan:'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 10}, page_content='Effective table data synthesizing. In Asian Conference on\\nMachine Learning , pp. 97–112. PMLR, 2021.\\nZhao, Z., Kunar, A., Birke, R., and Chen, L. Y . Ctab-\\ngan+: Enhancing tabular data synthesis. arXiv preprint\\narXiv:2204.00401 , 2022.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nA. Raw data processing\\nNumerical data Various methods have been proposed for\\nrepresenting numerical data as input to generative models\\nin the context of tabular data. The CTGAN and TV AE\\nmodels suggest the use of gaussian mixture models to en-\\ncode numerical values (Xu et al., 2019). On the other hand,\\nthe TabFormer model introduced quantization as a way to\\nencode numeric data (Padhi et al., 2021). However, these'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='approaches are lossy. As argued by Borisov et al. (2022),\\nthese lossy transformations may not be optimal.\\nIn our model, we adopt a fully text-based strategy in han-\\ndling numerical values. We apply a sequence of transforma-\\ntions that converts a column of numeric value into, possibly,\\nmulti-columnar data. We use the following transformation\\nof numerical columns. We also show the outcome of each\\ntransformation step on the sample numerical series below.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='For illustration, this example numerical-valued series\\n[1032.325345,10.291,-3.0]\\nis converted into\\n[“10”,“32”,“.3”,“3”]\\n[“00”,“10”,“.2”,“9”]\\n[“-0”,“03”,“.0”,“0”]\\n•We set a rounding resolution to normalize the size of\\nthe numerical values. For example, round to at most 2\\ndecimal places.\\n–[1032.33,10.29,-3.0]\\n• We then cast the values to string.\\n–[“1032.33”,“10.29”,“-3.0”]\\n•We identify the magnitude of the most signiﬁcant digit\\nof the largest value in the column by looking for the'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='location of the decimal point of the largest value. The\\nmagnitude of the most signiﬁcant digit for the largest\\nvalue in the example is 4.\\n–[“1032.33”,“10.29”,“-3.0”]\\n•We use the magnitude to left-align all the other values\\nin the data by padding them with leading zeros.\\n–[“1032.33”,“0010.29”,“00-3.0”]\\n•We then take the length of the longest string after this\\ntransformation and left-justify the data by padding ze-\\nros to the right of the values that are shorter than the\\nlongest string.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='longest string.\\n–[“1032.33”,“0010.29”,“00-3.00”]•Then, the negative sign for negative values is trans-\\nposed to the leftmost part of the string.\\n–[“1032.33”,“0010.29”,“-003.00”]\\n•Note that for integral values, we only perform the left\\nalignment by padding the values with leading zeros.\\nAfter this series of transformations, we tokenize the values\\ninto ﬁxed-length partitions. For the same example values,\\nsay we choose the partition size to be 2, we get the following\\ntokenized table.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='tokenized table.\\n[“10”,“32”,“.3”,“3”]\\n[“00”,“10”,“.2”,“9”]\\n[“-0”,“03”,“.0”,“0”]\\nThis transformation is done to mitigate the explosion of the\\nvocabulary if the numeric values are all distinct. We found\\nin our experiments that using single-character partitioning\\nworks best. We suppose that this effect is attributable to the\\ninherent regularization of generating an entire sequence of\\nnumbers one digit at a time.\\nDatetime data For date or time data types, we ﬁrst per-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='form a transformation of the raw data into Unix timestamp\\nrepresentation. This representation is then treated as regular\\nnumeric data; hence, we apply the data processing discussed\\nfor numeric data types.\\nCategorical data Unique values in categorical columns\\nare treated as unique tokens in the vocabulary. No additional\\nprocessing is done.\\nMissing values No transformation is done for missing\\nvalues present in the data. We let the model learn the dis-'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='tribution of the missing values. This strategy gives us the\\nﬂexibility to let the model impute or generate missing values\\nduring the sampling process.\\nInput data aggregation As illustrated above, the trans-\\nformation of numerical data types expands the dataset by\\npartitioning the string version of the values. As such, we\\ncombine the processed columns into modiﬁed tabular data.\\nWe use this modiﬁed tabular data as input for our models.\\nEach unique value in the new columns in this data will be'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 11}, page_content='mapped to a unique token in the vocabulary that is indepen-\\ndent of values in the other columns. This means that in the\\nillustrated numerical transformation shown above, the “1”\\nin the ﬁrst column will have a different token id than the “1”\\npresent in the third column.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 3. Summary of the datasets used in the experiments for non-relational tabular data.\\nAbbr Name # Train # Validation # Test # Num # Cat Task type\\nAB Abalone 2672 669 836 7 1 Regression\\nAD Adult ROC 26048 6513 16281 6 8 Binclass\\nBU Buddy 12053 3014 3767 4 5 Multiclass\\nCA California Housing 13209 3303 4128 8 0 Regression\\nDI Diabetes 491 123 154 8 0 Binclass'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='DI Diabetes 491 123 154 8 0 Binclass\\nFB Facebook Comments V olume 157638 19722 19720 50 1 Regression\\nB. Datasets\\nB.1. Non-relational tabular data\\nWe used six real-world datasets to assess the performance of our proposed model for generating realistic and useful synthetic\\ntabular data. The datasets are diverse with respect to the types of variables—mix of numerical and categorical data types—as'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='well as the number of variables in each dataset—ranging from 8 to 51 columns. The collection includes, Abalone (OpenML)2,\\nAdult (income estimation)3, Buddy (Kaggle)4, California Housing (real estate data)5, Diabetes (OpenML)6, and Facebook\\nComments7. Original source, copyright, and license information are available in the links in the footnote.\\nWe used the data splits by Kotelnikov et al. (2022) published in Tab-DDPM GitHub. Based on their pickled numpy data'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='dumps, we recreated the splits to create data frames that we can use for our experiments with REaLTabFormer and GReaT.\\nThe latter model expects contextual input from the column names.\\nWe also used the open-sourced optimized model parameters published in the above GitHub repo after reviewing the code,\\nand the correctness of the code relevant to producing the assets of interest has been conﬁrmed. We trained the TV AE,'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='CTABGAN+, and Tab-DDPM models from scratch using the parameters on each dataset.\\nB.2. Relational tabular data\\nTo test the REaLTabFormer in modeling relational datasets, we used two real-world data: the Rossmann store sales8dataset\\nand the Airbnb new user bookings9dataset.\\nWe created train and test splits. For the Rossmann dataset, we used 80% of the stores data and their associated sales records'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='for our training data. We used the remaining stores as the test data. We also limit the data used in the experiments from\\n2015-06 onwards spanning 2 months of sales data per store. In the Airbnb dataset, we considered a random sample of\\n10,000 users for the experiment. We take 8,000 as part of our training data, and we assessed the metrics and plots using the\\n2,000 users in the test data. We also limit the users considered to those having at most 50 sessions in the data.\\nC. Reproducibility'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='C. Reproducibility\\nWe used begreat==0.0.3 for the GReaT model. We used the Tab-DDPM GitHub repo version with this permanent\\nlink https://github.com/rotot0/tab-ddpm/tree/41f2415a378f1e8e8f4f5c3b8736521c0d47cf22. We used sdv==0.17.2\\nandsdmetrics==0.8.1 ; however, we ﬁxed a bug in the HyperTransformer implementation. We used\\ntransformers==4.25.1 andtorch==1.13.1 . We will open-source the REaLTabFormer package and experi-\\nments repository. We used Python version 3.9 .'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 12}, page_content='ments repository. We used Python version 3.9 .\\nWe ran our experiments on a standalone workstation with the following specs: 2x AMD EPYC 7H12 64-Core Processor, 2x\\nRTX 3090 GPU, and 1TB RAM running Ubuntu 20.04 LTS.\\n2Abalone (OpenML)\\n3Adult (income estimation)\\n4Buddy (Kaggle)\\n5California Housing (real estate data)\\n6Diabetes (OpenML)\\n7Facebook Comments\\n8Rossmann store sales\\n9Airbnb new user bookings'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 13}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nTable 4. Logistic detection measure for the generated parent, child, and merged tables by the Hierarchical Modeling Algorithm (HMA)\\nfrom SDV and the REaLTabFormer (RTF) models. This uses the logistic regression model as the detector.\\nDATASET TABLE SDV RTF\\nROSSMANNPARENT 78.67 ±6.79 92.75 ±4.28\\nCHILD 16.62 ±0.86 59.00 ±2.92\\nMERGED 12.00 ±0.73 50.69 ±2.41\\nAIRBNBPARENT 98.66 ±1.34 99.68 ±0.38'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 13}, page_content='AIRBNBPARENT 98.66 ±1.34 99.68 ±0.38\\nCHILD 0.00±0.00 26.33 ±0.78\\nMERGED 96.71 ±2.79 98.93 ±0.82\\nD. Other measures and results\\nWe also computed the logistic detection measure with the standard approach of using a logistic regression model. We ﬁnd\\nthat the logistic regression model appears to not provide reliable results Table 4. In particular, the scores returned by the\\nmodel are too high which is suspicious given that qualitative observation of the synthetic data hints at inaccuracies by both'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 13}, page_content='models in producing perfect alignment with the original data. These spurious results may be due to the model’s limited\\ncapacity of learning the structure of the data. While techniques can be applied to help the model detect non-linearities better,\\nwe opted to report the results using the random forest as the base detector since it naturally is able to learn non-linearities\\nand appears to give reasonable results.\\nD.1. Joint plots'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 13}, page_content='D.1. Joint plots\\nThe joint plot provides a qualitative assessment of the quality of the synthetic data generated by each model. We show in the\\nsequence of ﬁgures below the joint plots of two numerical variables in the datasets used.\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterOriginal\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterTVAE\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 13}, page_content='0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterCTABGAN+\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterT ab-DDPM\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterGReaT\\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6\\nShucked_weight0.00.10.20.30.40.50.60.7DiameterREaLT abFormer\\nFigure 5. Joint plot of two numerical variables in the Abalone data compared across the samples generated by the different models.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 14}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numOriginal\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numTVAE\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numCTABGAN+\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numT ab-DDPM\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numGReaT\\n10 20 30 40 50 60 70 80 90\\nage0246810121416education-numREaLT abFormer'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 14}, page_content='age0246810121416education-numREaLT abFormer\\nFigure 6. Joint plot of two numerical variables in the Adult data compared across the samples generated by the different models.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)Original\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)TVAE\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)CTABGAN+\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)T ab-DDPM\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)GReaT'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 14}, page_content='length(m)01020304050height(cm)GReaT\\n0.0 0.2 0.4 0.6 0.8 1.0\\nlength(m)01020304050height(cm)REaLT abFormer\\nFigure 7. Joint plot of two numerical variables in the Buddy data compared across the samples generated by the different models.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 15}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeOriginal\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeTVAE\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeCTABGAN+\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeT ab-DDPM\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeGReaT\\n124\\n 122\\n 120\\n 118\\n 116\\n 114\\nLongitude3436384042LatitudeREaLT abFormer'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 15}, page_content='114\\nLongitude3436384042LatitudeREaLT abFormer\\nFigure 8. Joint plot of two numerical variables in the California housing data compared across the samples generated by the different\\nmodels.\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseOriginal\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseTVAE\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseCTABGAN+\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseT ab-DDPM\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseGReaT'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 15}, page_content='BloodPressure050100150200GlucoseGReaT\\n0 20 40 60 80 100 120\\nBloodPressure050100150200GlucoseREaLT abFormer\\nFigure 9. Joint plot of two numerical variables in the Diabetes data compared across the samples generated by the different models.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers\\nD.2. Distance to closest record (DCR) distribution\\nWe present earlier the distance to closest record (DCR) distribution, Equation 2, as part of our proposed strategy to detect\\noverﬁtting in training the REaLTabFormer model. Here, we use the DCR distribution to visually assess whether the\\ngenerative models create exact copies of observations from the training data. We also show the DCR distribution of the real'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='test data as a reference.\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityOriginal\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityTVAE\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityCTABGAN+\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityT ab-DDPM\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityGReaT\\n0.0 0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityREaLT abFormer'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='Figure 10. Distance to closest record (DCR) distributions of the different models for the Abalone data.\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityOriginal\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityTVAE\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityCTABGAN+\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityT ab-DDPM\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityGReaT\\n0 1 2 3 4 5\\nDCR0.00.10.20.30.40.5DensityREaLT abFormer\\nFigure 11. Distance to closest record (DCR) distributions of the different models for the Adult data.'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='0 1 2 3\\nDCR0.00.20.40.60.8DensityOriginal\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityTVAE\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityCTABGAN+\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityT ab-DDPM\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityGReaT\\n0 1 2 3\\nDCR0.00.20.40.60.8DensityREaLT abFormer\\nFigure 12. Distance to closest record (DCR) distributions of the different models for the Buddy data.\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityOriginal\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityTVAE\\n0.5 1.0 1.5'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityCTABGAN+\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityT ab-DDPM\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityGReaT\\n0.5 1.0 1.5\\nDCR0.000.250.500.751.001.251.50DensityREaLT abFormer\\nFigure 13. Distance to closest record (DCR) distributions of the different models for the California housing data.\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityOriginal\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityTVAE\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityCTABGAN+\\n1 2 3 4'),\n",
       " Document(metadata={'source': 'RealTabformer.pdf', 'page': 16}, page_content='1 2 3 4\\nDCR0.00.20.40.60.8DensityCTABGAN+\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityT ab-DDPM\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityGReaT\\n1 2 3 4\\nDCR0.00.20.40.60.8DensityREaLT abFormer\\nFigure 14. Distance to closest record (DCR) distributions of the different models for the Diabetes data.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 50)\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='REaLTabFormer: Generating Realistic Relational\n",
      "and Tabular Data using Transformers\n",
      "Aivin V . Solatorio1Olivier Dupriez1\n",
      "Abstract\n",
      "Tabular data is a common form of organizing data.\n",
      "Multiple models are available to generate syn-\n",
      "thetic tabular datasets where observations are in-\n",
      "dependent, but few have the ability to produce\n",
      "relational datasets. Modeling relational data is\n",
      "challenging as it requires modeling both a “par-\n",
      "ent” table and its relationships across tables. We' metadata={'source': 'RealTabformer.pdf', 'page': 0}\n",
      "page_content='introduce REaLTabFormer (Realistic Relational\n",
      "and Tabular Transformer), a tabular and relational\n",
      "synthetic data generation model. It ﬁrst creates a\n",
      "parent table using an autoregressive GPT-2 model,\n",
      "then generates the relational dataset conditioned\n",
      "on the parent table using a sequence-to-sequence\n",
      "(Seq2Seq) model. We implement target masking\n",
      "to prevent data copying and propose the Qδstatis-\n",
      "tic and statistical bootstrapping to detect over-\n",
      "ﬁtting. Experiments using real-world datasets' metadata={'source': 'RealTabformer.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[0])\n",
    "print(final_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "docs = TextLoader(\"test.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Freemium Model: Offer basic features for free but charge for advanced analytics, detailed insights,'\n",
      "page_content='insights, or priority updates.'\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "    test = f.read()\n",
    "test\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap = 10)\n",
    "text = text_splitter.create_documents([test])\n",
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
